# MDPs and Multi-Armed Bandits: An Overview

A survey paper covering Markov decision processes, bandit problems, and related models. The paper is being refactored from a single monolithic document (V1) into a collection of standalone monographs (V2).

## Repository Structure

```
LaTeX/MDPS Overview/
  V1/                     -- Original paper (archived, read-only reference)
    MDPs and Multi-Arm Bandits.tex   -- Final V1 source
    MDPs and Multi-Arm Bandits v{1,2,3}.tex -- Earlier drafts
    critique{1,2}.md, critique-response.md  -- Peer review notes
  monographs/             -- V2 refactor: one folder per topic
    02-markov-processes/
    03-markov-decision-processes/
    04-partially-observable-markov-decision-processes/
    05-multi-armed-bandit-problems/
    06-contextual-bandit-problems/
    07-restless-bandit-problems/
    08-beyond-the-scope-of-this-paper/
```

## V1 Sections

1. Introduction
2. Markov Processes
3. Markov Decision Processes
4. Partially Observable MDPs
5. Multi-Armed Bandit Problems
6. Contextual Bandit Problems
7. Restless Bandit Problems
8. Beyond the Scope of This Paper
9. Conclusion

## Other Files

| File | Description |
|------|-------------|
| `linucb-example.tex` | Standalone LinUCB worked example |
| `asset-mdp-diagram.md` | MDP diagram notes |
| `review.md` | Review notes |
| `LaTeX/Waterworks Problem/` | Waterworks MDP model |
| `LaTeX/Waterworks - Alt Solution/` | Alternate waterworks solution |

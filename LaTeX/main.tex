\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1,T2A]{fontenc}
\usepackage[russian,english]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage[unicode]{hyperref}
\usepackage{tikz}
\usepackage{listings}
\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\bfseries,
  commentstyle=\itshape,
  numbers=none,
  frame=single,
  breaklines=true,
  captionpos=b,
  columns=fullflexible,
  showstringspaces=false,
  literate={_}{\_}1
}
\usetikzlibrary{automata, positioning, arrows.meta}

\geometry{margin=1in}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\title{Markov Decision Processes and Multi-Armed Bandits: Overview and Worked Examples}
\author{Gregory Zancewicz\thanks{With assistance from Anthropic Claude Opus 4.5 and OpenAI GPT-5.2 AI agents.}}
\date{February 3, 2026}

\begin{document}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\maketitle
\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}

\begin{abstract}
This paper provides a self-contained overview of five core model classes in Markov-based sequential decision-making: Markov processes, Markov decision processes (MDPs), partially observable MDPs (POMDPs), multi-armed bandits, and restless bandits. The models are arranged in a progression of increasing generality, beginning with passive stochastic dynamics and culminating in problems where multiple evolving subsystems must be managed under combinatorial constraints. For each model class we present a formal definition, the principal theoretical results---including the Bellman equation, Bayesian belief updates, regret bounds, and the Whittle index---and one or more fully worked numerical examples with accompanying Python code. Bandit strategies covered include $\varepsilon$-greedy, UCB1, Thompson Sampling, the Gittins index, successive elimination, KL-UCB, and EXP3. We also briefly survey hidden Markov models, contextual bandits, Markov games, constrained MDPs, semi-Markov decision processes, and model-free reinforcement learning. A glossary of all technical terms is provided. The paper is intended as an accessible yet rigorous entry point for researchers and practitioners new to the field.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
\label{sec:intro}

Many problems in engineering, operations research, economics, and artificial intelligence share a common structure: an agent must make a sequence of decisions under uncertainty, where each decision influences future states of the world and the rewards that follow. The mathematical framework for such problems rests on the theory of Markov processes and their controlled extensions, a body of work spanning over a century---from Markov's original study of dependent random variables \cite{markov1906} through Bellman's dynamic programming \cite{bellman1957} to modern bandit algorithms deployed in clinical trials, online advertising, and autonomous systems.

This paper provides a self-contained overview of five core model classes arranged in a progression of increasing generality:

\begin{enumerate}
  \item \textbf{Markov Processes} (Section~\ref{sec:mc}). A system evolves randomly among a finite set of states according to fixed transition probabilities, with no external control. The transition matrix and stationary distribution are the central objects.

  \item \textbf{Markov Decision Processes} (Section~\ref{sec:mdp}). An agent chooses actions that influence both state transitions and rewards. The Bellman equation characterizes the optimal policy, and value iteration provides a constructive solution method.

  \item \textbf{Partially Observable MDPs} (Section~\ref{sec:pomdp}). The agent can no longer observe the state directly and must maintain a belief distribution updated via Bayes' rule. The POMDP reduces to a continuous-state MDP over the belief simplex, at the cost of greatly increased computational complexity.

  \item \textbf{Multi-Armed Bandits} (Section~\ref{sec:mab}). The exploration--exploitation tradeoff is isolated in its purest form: a single-state problem where the agent must learn which arm is best by trying them. We cover $\varepsilon$-greedy, UCB1, Thompson Sampling, the Gittins index, successive elimination, KL-UCB, and EXP3, together with the Lai--Robbins regret lower bound.

  \item \textbf{Restless Bandits} (Section~\ref{sec:restless}). The classical bandit assumption that unplayed arms are frozen is removed: every arm evolves at every time step. The Whittle index policy provides a tractable heuristic for this PSPACE-hard problem.
\end{enumerate}

Section~\ref{sec:beyond} briefly surveys several related model classes---hidden Markov models, contextual bandits, Markov games, constrained MDPs, semi-Markov decision processes, and model-free reinforcement learning---that lie beyond the scope of this paper but occupy adjacent regions of the same theoretical landscape.

Each section follows a uniform structure: a notation table listing all symbols used in that section, a formal definition, the key theoretical results, one or more worked examples with explicit numerical computation, and a summary containing the key equations and relevant applications. Where appropriate, Python code is provided to verify analytical solutions via value iteration or simulation. The paper concludes with a glossary of all technical terms.

This paper is pedagogical in intent; it presents no new results but aims to collect, unify, and illustrate the core ideas in a single self-contained reference. The intended audience is researchers and practitioners who seek a concise, rigorous, yet accessible entry point into Markov-based sequential decision-making. Familiarity with linear algebra and basic probability is assumed; no prior exposure to dynamic programming or bandit theory is required.

\section{Markov Processes}
\label{sec:mc}

A \textbf{Markov process} (or Markov chain) is a stochastic process satisfying the \emph{Markov property}: the conditional probability of future states depends only on the present state, not on the sequence of states that preceded it. This idea was first formalized by A.\ A.\ Markov in his study of dependent random variables \cite{markov1906}.

\subsection{Notation}

\begin{tabular}{ll}
  \textbf{Symbol} & \textbf{Description} \\
  \hline
  $\mathcal{S} = \{s_1, \dots, s_n\}$ & Finite set of states \\
  $t$ & Discrete time index, $t = 0, 1, 2, \dots$ \\
  $X_t$ & State occupied at time $t$ \\
  $p_{ij}$ & Transition probability from state $s_i$ to state $s_j$ \\
  $n$ & Number of states, $|\mathcal{S}|$ \\
  $\mathbf{P}$ & Transition matrix, $[\mathbf{P}]_{ij} = p_{ij}$ \\
  $\boldsymbol{\mu}_t$ & Row vector distribution over states at time $t$ \\
  $\boldsymbol{\mu}^*$ & Stationary distribution, $\boldsymbol{\mu}^* = \boldsymbol{\mu}^* \mathbf{P}$ \\
  $\alpha$ & Transition probability $s_1 \to s_2$ (two-state example) \\
  $\beta$ & Transition probability $s_2 \to s_1$ (two-state example) \\
\end{tabular}

\subsection{Definitions and Notation}

Let $\mathcal{S} = \{s_1, s_2, \dots, s_n\}$ denote a finite set of \textbf{states}. Each state is simply an element of this set; it may represent a concrete condition (e.g., ``sunny'' or ``rainy''), a configuration of a system, or, in some applications, a random variable---but need not in general. A discrete-time Markov chain is a process that, at each time step $t = 0, 1, 2, \dots$, occupies some state $X_t \in \mathcal{S}$ and transitions to a next state. The defining assumption---the \emph{Markov property}---is that the probability of the next state depends only on the current state, not on how the process arrived there:
\begin{equation}
  P(X_{t+1} = s_j \mid X_t = s_i, X_{t-1}, \dots, X_0) = P(X_{t+1} = s_j \mid X_t = s_i) \triangleq p_{ij}.
\end{equation}
That is, conditioning on the full history (left side) yields the same probability as conditioning on the present state alone (right side). We denote this common value $p_{ij}$, the \textbf{transition probability} from state $s_i$ to state $s_j$. The transition probabilities satisfy
\begin{equation}
  p_{ij} \geq 0 \quad \text{and} \quad \sum_{j=1}^{n} p_{ij} = 1 \quad \text{for all } i.
\end{equation}

These probabilities are collected in the \textbf{transition matrix}
\begin{equation}
  \mathbf{P} = \begin{pmatrix} p_{11} & p_{12} & \cdots & p_{1n} \\ p_{21} & p_{22} & \cdots & p_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ p_{n1} & p_{n2} & \cdots & p_{nn} \end{pmatrix},
\end{equation}
where each row sums to one. The distribution over states at time $t$ is a row vector $\boldsymbol{\mu}_t$, and the evolution of the chain is given by $\boldsymbol{\mu}_{t+1} = \boldsymbol{\mu}_t \mathbf{P}$.

A distribution $\boldsymbol{\mu}^*$ is called a \textbf{stationary distribution} if $\boldsymbol{\mu}^* = \boldsymbol{\mu}^* \mathbf{P}$.

\subsection{Two-State Example}

Consider a Markov chain with two states, $\mathcal{S} = \{s_1, s_2\}$, and transition matrix
\begin{equation}
  \mathbf{P} = \begin{pmatrix} 1 - \alpha & \alpha \\ \beta & 1 - \beta \end{pmatrix},
\end{equation}
where $\alpha = P(X_{t+1} = s_2 \mid X_t = s_1)$ and $\beta = P(X_{t+1} = s_1 \mid X_t = s_2)$. The corresponding state diagram is shown in Figure~\ref{fig:two-state}.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[->, >=Stealth, shorten >=1pt, auto, node distance=4cm,
                      thick, every state/.style={fill=gray!10, minimum size=1cm}]
    \node[state] (s1) {$s_1$};
    \node[state] (s2) [right=of s1] {$s_2$};

    \path (s1) edge [bend left=20] node {$\alpha$} (s2)
          (s2) edge [bend left=20] node {$\beta$} (s1)
          (s1) edge [loop left]    node {$1-\alpha$} (s1)
          (s2) edge [loop right]   node {$1-\beta$} (s2);
  \end{tikzpicture}
  \caption{A two-state Markov chain with transition probabilities $\alpha$ and $\beta$.}
  \label{fig:two-state}
\end{figure}

Provided $\alpha, \beta > 0$, this chain is ergodic\footnote{A Markov chain is \emph{ergodic} if it is irreducible (every state is reachable from every other state) and aperiodic (the chain does not cycle among subsets of states with a fixed period). An ergodic chain converges to a unique stationary distribution regardless of the initial state.} and admits the stationary\footnote{A distribution that, once reached, remains unchanged under further transitions of the chain.} distribution
\begin{equation}
  \boldsymbol{\mu}^* = \left(\frac{\beta}{\alpha + \beta},\; \frac{\alpha}{\alpha + \beta}\right).
\end{equation}

\subsection{Summary}

A Markov process models a system that transitions randomly among a finite set of states, with the defining property that the future depends only on the present state---not on the path taken to reach it. The transition matrix $\mathbf{P}$ fully characterizes the dynamics, and the distribution over states evolves by left-multiplication: $\boldsymbol{\mu}_{t+1} = \boldsymbol{\mu}_t \mathbf{P}$. Under mild conditions (ergodicity), the chain converges to a unique stationary distribution regardless of the starting state. Markov processes provide the probabilistic foundation for all models in the subsequent sections: MDPs add actions and rewards, POMDPs add partial observability, and bandit models isolate the exploration--exploitation tradeoff.

\subsubsection{Key Equations}

Markov property:
\begin{equation*}
  P(X_{t+1} = s_j \mid X_t = s_i, X_{t-1}, \dots, X_0) = P(X_{t+1} = s_j \mid X_t = s_i) \triangleq p_{ij}.
\end{equation*}

Distribution evolution:
\begin{equation*}
  \boldsymbol{\mu}_{t+1} = \boldsymbol{\mu}_t \mathbf{P}.
\end{equation*}

Stationary distribution:
\begin{equation*}
  \boldsymbol{\mu}^* = \boldsymbol{\mu}^* \mathbf{P}.
\end{equation*}

Stationary distribution (two-state chain):
\begin{equation*}
  \boldsymbol{\mu}^* = \left(\frac{\beta}{\alpha + \beta},\; \frac{\alpha}{\alpha + \beta}\right).
\end{equation*}

\subsubsection{Relevant Applications}
\begin{itemize}
  \item Weather modeling, where atmospheric conditions transition among discrete states (e.g., sunny, cloudy, rainy) with estimated probabilities.
  \item Queueing systems, where the number of customers in a queue evolves as a Markov chain governed by arrival and service rates.
  \item PageRank and web link analysis, where the stationary distribution of a random walk on the web graph determines page importance.
  \item Population genetics, where allele frequencies evolve across generations via random drift modeled as a Markov chain.
  \item Speech and language modeling, where sequences of phonemes or words are generated by hidden or observable Markov models.
\end{itemize}

\section{Markov Decision Processes}
\label{sec:mdp}

A Markov decision process extends the Markov chain of Section~\ref{sec:mc} by introducing \emph{actions} and \emph{rewards}. Where a Markov chain evolves passively according to fixed transition probabilities, an MDP allows an agent to \emph{choose} an action at each time step, thereby influencing both the transition to the next state and the reward received. This formulation originates with Bellman \cite{bellman1957}, who studied the asymptotic behavior of nonlinear recurrence relations arising from dynamic programming.

\subsection{Notation}

\begin{tabular}{ll}
  \textbf{Symbol} & \textbf{Description} \\
  \hline
  $\mathcal{A} = \{a_1, \dots, a_m\}$ & Finite set of actions \\
  $p_{ij}(a)$ & Transition probability from $s_i$ to $s_j$ under action $a$ \\
  $r(s_i, a)$ & Immediate reward for taking action $a$ in state $s_i$ \\
  $\gamma$ & Discount factor, $\gamma \in [0, 1)$ \\
  $\pi$ & Policy, a mapping $\pi : \mathcal{S} \to \mathcal{A}$ \\
  $V : \mathcal{S} \to \mathbb{R}$ & Value function \\
  $V^\pi(s_i)$ & Value of state $s_i$ under policy $\pi$ \\
  $V^*(s_i)$ & Optimal value of state $s_i$ \\
  $\pi^*$ & Optimal policy \\
  $f_N(s_i)$ & Optimal return over $N$ remaining stages (Bellman's notation) \\
  $b(s_i, a)$ & Immediate return (Bellman's notation) \\
  $a_{ij}(a)$ & Transition coefficients (Bellman's notation) \\
\end{tabular}

\subsection{Formal Definition}

A Markov decision process is defined by the tuple $(\mathcal{S}, \mathcal{A}, p, r, \gamma)$:

\begin{itemize}
  \item $\mathcal{S} = \{s_1, s_2, \dots, s_n\}$: a finite set of \textbf{states}, as in Section~\ref{sec:mc}.
  \item $\mathcal{A} = \{a_1, a_2, \dots, a_m\}$: a finite set of \textbf{actions} (or decisions) available to the agent.
  \item $p_{ij}(a)$: the \textbf{transition probability} of moving from state $s_i$ to state $s_j$ when action $a$ is taken. For each state $s_i$ and action $a$,
  \begin{equation}
    p_{ij}(a) \geq 0 \quad \text{and} \quad \sum_{j=1}^{n} p_{ij}(a) = 1.
  \end{equation}
  Note that the transition probabilities now depend on the chosen action---this is the key departure from the passive Markov chain, where $p_{ij}$ was fixed.
  \item $r(s_i, a)$: the \textbf{immediate reward} (or return) received when action $a$ is taken in state $s_i$.
  \item $\gamma \in [0, 1)$: a \textbf{discount factor} that weights future rewards relative to immediate ones.
\end{itemize}

\subsection{Policies}

A \textbf{policy} $\pi : \mathcal{S} \to \mathcal{A}$ is a rule that specifies which action to take in each state. Under a fixed policy $\pi$, the MDP reduces to an ordinary Markov chain with transition probabilities $p_{ij}(\pi(s_i))$.

\subsection{The Value Function and Bellman's Equation}

Following Bellman's approach, we seek a \textbf{value function} $V : \mathcal{S} \to \mathbb{R}$ that assigns to each state the maximum total discounted reward obtainable from that state onward. Formally, for a policy $\pi$ the value of state $s_i$ is
\begin{equation}
  V^\pi(s_i) = \mathbb{E}\!\left[\sum_{t=0}^{\infty} \gamma^t\, r(X_t, \pi(X_t)) \;\middle|\; X_0 = s_i\right],
\end{equation}
where $X_t \in \mathcal{S}$ is the state at time $t$ and the expectation is over the stochastic transitions. The \textbf{optimal value function} is then $V^*(s_i) = \max_\pi V^\pi(s_i)$. Bellman showed that $V^*$ must satisfy the recurrence relation
\begin{equation}
  V^*(s_i) = \max_{a \in \mathcal{A}} \left[ r(s_i, a) + \gamma \sum_{j=1}^{n} p_{ij}(a)\, V^*(s_j) \right], \quad i = 1, 2, \dots, n.
\end{equation}
This is the \textbf{Bellman equation}. It expresses a principle of optimality: the value of a state equals the best immediate reward plus the discounted expected value of the successor state, maximized over all available actions.

In Bellman's original notation \cite{bellman1957}, this appears as the nonlinear recurrence
\begin{equation}
  f_N(s_i) = \max_{a} \left[ b(s_i, a) + \sum_{j=1}^{n} a_{ij}(a)\, f_{N-1}(s_j) \right],
\end{equation}
where $f_N(s_i)$ is the optimal return over $N$ remaining stages, $b(s_i, a)$ is the immediate return\footnote{Bellman's use of $b$ for immediate return should not be confused with the belief state $\mathbf{b}$ introduced in Section~\ref{sec:pomdp}. We reproduce his original notation here for fidelity to the source.}, and $a_{ij}(a)$ are the transition coefficients. Our formulation above corresponds to the infinite-horizon, discounted case obtained by letting $N \to \infty$ with discount factor $\gamma$.

The optimal policy $\pi^*$ is then the one that achieves the maximum in the Bellman equation at every state:
\begin{equation}
  \pi^*(s_i) = \arg\max_{a \in \mathcal{A}} \left[ r(s_i, a) + \gamma \sum_{j=1}^{n} p_{ij}(a)\, V^*(s_j) \right].
\end{equation}

\subsection{Examples}

\subsubsection{Example 1: Basic Binary MDP}

Consider an MDP with two states $\mathcal{S} = \{s_1, s_2\}$ and two actions $\mathcal{A} = \{a_1, a_2\}$. The transition probabilities and rewards are:

\begin{center}
\begin{tabular}{cccc}
  State & Action & Transitions & Reward \\
  \hline
  $s_1$ & $a_1$ & $p_{11}(a_1) = 0.8,\; p_{12}(a_1) = 0.2$ & $r(s_1, a_1) = 5$ \\
  $s_1$ & $a_2$ & $p_{11}(a_2) = 0.3,\; p_{12}(a_2) = 0.7$ & $r(s_1, a_2) = 1$ \\
  $s_2$ & $a_1$ & $p_{21}(a_1) = 0.6,\; p_{22}(a_1) = 0.4$ & $r(s_2, a_1) = 2$ \\
  $s_2$ & $a_2$ & $p_{21}(a_2) = 0.1,\; p_{22}(a_2) = 0.9$ & $r(s_2, a_2) = 8$ \\
\end{tabular}
\end{center}

The corresponding decision diagram is shown in Figure~\ref{fig:mdp-two-state}. Each state node (solid outline) branches into action nodes (dotted outline), which in turn lead probabilistically to successor states.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[->, >=Stealth, shorten >=1pt, auto,
                      thick,
                      state/.style={draw, circle, fill=gray!10, minimum size=1cm},
                      action/.style={draw, dotted, rectangle, rounded corners=3pt,
                                     minimum size=0.7cm, inner sep=2pt, fill=white}]

    % State nodes
    \node[state] (s1) {$s_1$};
    \node[state] (s2) [right=8cm of s1] {$s_2$};

    % Action nodes for s1
    \node[action] (s1a1) [above right=1.5cm and 1.5cm of s1] {$s_1, a_1$};
    \node[action] (s1a2) [below right=1.5cm and 1.5cm of s1] {$s_1, a_2$};

    % Action nodes for s2
    \node[action] (s2a1) [above left=1.5cm and 1.5cm of s2] {$s_2, a_1$};
    \node[action] (s2a2) [below left=1.5cm and 1.5cm of s2] {$s_2, a_2$};

    % Edges: states to action nodes
    \path (s1) edge (s1a1)
          (s1) edge (s1a2)
          (s2) edge (s2a1)
          (s2) edge (s2a2);

    % Edges: action nodes to successor states (s1's actions)
    \path (s1a1) edge [bend left=15] node[above right] {$0.8$} (s1)
          (s1a1) edge node[above] {$0.2$} (s2)
          (s1a2) edge [bend right=15] node[below right] {$0.3$} (s1)
          (s1a2) edge node[below] {$0.7$} (s2);

    % Edges: action nodes to successor states (s2's actions)
    \path (s2a1) edge node[above] {$0.6$} (s1)
          (s2a1) edge [bend left=15] node[above left] {$0.4$} (s2)
          (s2a2) edge node[below] {$0.1$} (s1)
          (s2a2) edge [bend right=15] node[below left] {$0.9$} (s2);

    % Reward labels
    \node[above=0.1cm of s1a1, font=\small\itshape] {$r=5$};
    \node[below=0.1cm of s1a2, font=\small\itshape] {$r=1$};
    \node[above=0.1cm of s2a1, font=\small\itshape] {$r=2$};
    \node[below=0.1cm of s2a2, font=\small\itshape] {$r=8$};

  \end{tikzpicture}
  \caption{A two-state MDP. Solid circles are states; dotted rectangles are action (decision) nodes labeled $(s, a)$. Edges from action nodes to states are labeled with transition probabilities. Rewards are shown in italics above or below each action node.}
  \label{fig:mdp-two-state}
\end{figure}

With discount factor $\gamma = 0.9$, the Bellman equation gives the system
\begin{align}
  V^*(s_1) &= \max\!\Big\{ \underbrace{5 + 0.9\big[0.8\, V^*(s_1) + 0.2\, V^*(s_2)\big]}_{a_1},\;
               \underbrace{1 + 0.9\big[0.3\, V^*(s_1) + 0.7\, V^*(s_2)\big]}_{a_2} \Big\}, \\
  V^*(s_2) &= \max\!\Big\{ \underbrace{2 + 0.9\big[0.6\, V^*(s_1) + 0.4\, V^*(s_2)\big]}_{a_1},\;
               \underbrace{8 + 0.9\big[0.1\, V^*(s_1) + 0.9\, V^*(s_2)\big]}_{a_2} \Big\}.
\end{align}

To solve, we guess that the optimal policy is $\pi^*(s_1) = a_1$ and $\pi^*(s_2) = a_2$ (choosing the high-reward action in each state) and verify. Under this candidate the Bellman equations become linear:
\begin{align}
  V^*(s_1) &= 5 + 0.9\big[0.8\, V^*(s_1) + 0.2\, V^*(s_2)\big], \\
  V^*(s_2) &= 8 + 0.9\big[0.1\, V^*(s_1) + 0.9\, V^*(s_2)\big].
\end{align}
Rearranging,
\begin{align}
  (1 - 0.72)\, V^*(s_1) - 0.18\, V^*(s_2) &= 5, \\
  -0.09\, V^*(s_1) + (1 - 0.81)\, V^*(s_2) &= 8,
\end{align}
i.e.,
\begin{align}
  0.28\, V^*(s_1) - 0.18\, V^*(s_2) &= 5, \\
  -0.09\, V^*(s_1) + 0.19\, V^*(s_2) &= 8.
\end{align}
Solving yields $V^*(s_1) \approx 64.59$ and $V^*(s_2) \approx 72.70$. To verify optimality, we check that the alternative action does not yield a higher value at either state:
\begin{align}
  \text{At } s_1\!: \quad a_2 &\implies 1 + 0.9\big[0.3(64.59) + 0.7(72.70)\big] \approx 64.24 < 64.59, \\
  \text{At } s_2\!: \quad a_1 &\implies 2 + 0.9\big[0.6(64.59) + 0.4(72.70)\big] \approx 63.05 < 72.70.
\end{align}
Since neither alternative improves the value, the candidate policy is confirmed optimal:
\begin{equation}
  \pi^*(s_1) = a_1, \qquad \pi^*(s_2) = a_2.
\end{equation}
The agent should take action $a_1$ (high immediate reward, likely stay in $s_1$) when in state $s_1$, and action $a_2$ (high immediate reward, likely stay in $s_2$) when in state $s_2$.

The following Python code verifies this solution via value iteration:

\begin{lstlisting}[caption={Value iteration for the two-state MDP example.}]
import numpy as np

V = np.array([0.0, 0.0])
gamma = 0.9

for i in range(10000):
    V1_a1 = 5 + gamma * (0.8 * V[0] + 0.2 * V[1])
    V1_a2 = 1 + gamma * (0.3 * V[0] + 0.7 * V[1])
    V2_a1 = 2 + gamma * (0.6 * V[0] + 0.4 * V[1])
    V2_a2 = 8 + gamma * (0.1 * V[0] + 0.9 * V[1])

    V_new = np.array([max(V1_a1, V1_a2), max(V2_a1, V2_a2)])
    if np.max(np.abs(V_new - V)) < 1e-10:
        print(f"Converged at iteration {i}")
        break
    V = V_new

print(f"V*(s1) = {V[0]:.2f}")  # 64.59
print(f"V*(s2) = {V[1]:.2f}")  # 72.70
\end{lstlisting}

\subsubsection{Example 2: Simple Asset Maintenance MDP}

Consider an asset that is either operational or failed. The agent must decide at each time step whether to do nothing or replace the asset. The MDP is defined by $\mathcal{S} = \{\text{Op}, \text{Fail}\}$, $\mathcal{A} = \{\text{Do Nothing}, \text{Replace}\}$, and discount factor $\gamma = 0.9$. The state value of being operational is $+1$ unit per period and the penalty for being in a failed state is $-10$ units per period. Replacement costs $5$ units regardless of the current state; doing nothing is free. The net reward is therefore $r(s, a) = (\text{state value}) - (\text{action cost})$:

\begin{center}
\begin{tabular}{cccc}
  State & Action & Transitions & Reward \\
  \hline
  Op & Do Nothing & $p(\text{Op}\!\to\!\text{Op}) = 0.9,\; p(\text{Op}\!\to\!\text{Fail}) = 0.1$ & $r = 1$ \\
  Op & Replace & $p(\text{Op}\!\to\!\text{Op}) = 1.0$ & $r = -4$ \\
  Fail & Do Nothing & $p(\text{Fail}\!\to\!\text{Fail}) = 1.0$ & $r = -10$ \\
  Fail & Replace & $p(\text{Fail}\!\to\!\text{Op}) = 1.0$ & $r = -15$ \\
\end{tabular}
\end{center}

An operational asset that is left alone fails with probability $0.1$ each period; once failed, it remains failed until replaced. Replacement always restores the asset to operational.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[->, >=Stealth, shorten >=1pt, auto,
                      thick,
                      state/.style={draw, circle, fill=gray!10, minimum size=1cm},
                      action/.style={draw, dotted, rectangle, rounded corners=3pt,
                                     minimum size=0.7cm, inner sep=2pt, fill=white}]

    % State nodes
    \node[state] (op) {Op};
    \node[state] (fail) [right=8cm of op] {Fail};

    % Action nodes for Op
    \node[action] (op_dn) [above right=1.5cm and 1.5cm of op] {Op, DN};
    \node[action] (op_rep) [below right=1.5cm and 1.5cm of op] {Op, Rep};

    % Action nodes for Fail
    \node[action] (fail_dn) [above left=1.5cm and 1.5cm of fail] {Fail, DN};
    \node[action] (fail_rep) [below left=1.5cm and 1.5cm of fail] {Fail, Rep};

    % Edges: states to action nodes
    \path (op) edge (op_dn)
          (op) edge (op_rep)
          (fail) edge (fail_dn)
          (fail) edge (fail_rep);

    % Edges: Op actions
    \path (op_dn) edge [bend left=15] node[above right] {$0.9$} (op)
          (op_dn) edge node[above] {$0.1$} (fail)
          (op_rep) edge [bend right=15] node[below right] {$1.0$} (op);

    % Edges: Fail actions
    \path (fail_dn) edge [bend left=15] node[above left] {$1.0$} (fail)
          (fail_rep) edge node[below] {$1.0$} (op);

    % Reward labels
    \node[above=0.1cm of op_dn, font=\small\itshape] {$r=1$};
    \node[below=0.1cm of op_rep, font=\small\itshape] {$r=-4$};
    \node[above=0.1cm of fail_dn, font=\small\itshape] {$r=-10$};
    \node[below=0.1cm of fail_rep, font=\small\itshape] {$r=-15$};

  \end{tikzpicture}
  \caption{Asset maintenance MDP. DN = Do Nothing, Rep = Replace.}
  \label{fig:mdp-asset}
\end{figure}

The Bellman equation gives:
\begin{align}
  V^*(\text{Op}) &= \max\!\Big\{ \underbrace{1 + 0.9\big[0.9\, V^*(\text{Op}) + 0.1\, V^*(\text{Fail})\big]}_{\text{Do Nothing}},\;
               \underbrace{-4 + 0.9\, V^*(\text{Op})}_{\text{Replace}} \Big\}, \\
  V^*(\text{Fail}) &= \max\!\Big\{ \underbrace{-10 + 0.9\, V^*(\text{Fail})}_{\text{Do Nothing}},\;
               \underbrace{-15 + 0.9\, V^*(\text{Op})}_{\text{Replace}} \Big\}.
\end{align}

The intuition is clear: an operational asset should be left alone (avoiding the unnecessary replacement cost), while a failed asset should be replaced (the one-time cost of $5$ is far preferable to accumulating $-10$ per period indefinitely). We guess $\pi^*(\text{Op}) = \text{Do Nothing}$ and $\pi^*(\text{Fail}) = \text{Replace}$ and verify. Under this candidate:
\begin{align}
  V^*(\text{Op}) &= 1 + 0.9\big[0.9\, V^*(\text{Op}) + 0.1\, V^*(\text{Fail})\big], \\
  V^*(\text{Fail}) &= -15 + 0.9\, V^*(\text{Op}).
\end{align}
Rearranging,
\begin{align}
  0.19\, V^*(\text{Op}) - 0.09\, V^*(\text{Fail}) &= 1, \\
  -0.9\, V^*(\text{Op}) + V^*(\text{Fail}) &= -15.
\end{align}
Solving yields $V^*(\text{Op}) \approx -3.21$ and $V^*(\text{Fail}) \approx -17.89$. Both values are negative because maintenance costs and failure penalties erode returns over the infinite horizon. To verify optimality:
\begin{align}
  \text{At Op:} \quad \text{Replace} &\implies -4 + 0.9(-3.21) \approx -6.89 < -3.21, \\
  \text{At Fail:} \quad \text{Do Nothing} &\implies -10 + 0.9(-17.89) \approx -26.10 < -17.89.
\end{align}
Neither alternative improves the value, confirming the optimal policy:
\begin{equation}
  \pi^*(\text{Op}) = \text{Do Nothing}, \qquad \pi^*(\text{Fail}) = \text{Replace}.
\end{equation}

The following Python code verifies this solution via value iteration:

\begin{lstlisting}[caption={Value iteration for the asset maintenance MDP.}]
import numpy as np

V = np.array([0.0, 0.0])  # [V(Op), V(Fail)]
gamma = 0.9

for i in range(10000):
    V_op_dn  = 1  + gamma * (0.9 * V[0] + 0.1 * V[1])
    V_op_rep = -4 + gamma * (1.0 * V[0])
    V_fl_dn  = -10 + gamma * (1.0 * V[1])
    V_fl_rep = -15 + gamma * (1.0 * V[0])

    V_new = np.array([max(V_op_dn, V_op_rep), max(V_fl_dn, V_fl_rep)])
    if np.max(np.abs(V_new - V)) < 1e-10:
        print(f"Converged at iteration {i}")
        break
    V = V_new

print(f"V*(Op)   = {V[0]:.2f}")  # -3.21
print(f"V*(Fail) = {V[1]:.2f}")  # -17.89
\end{lstlisting}

\subsection{Summary}

The Markov decision process augments the passive Markov chain with actions and rewards, giving an agent the ability to influence state transitions and accumulate returns. The Bellman equation provides a recursive characterization of the optimal value function, expressing the principle of optimality: the best strategy from any state consists of taking the best immediate action and then continuing optimally. Solving the Bellman equation---via value iteration, policy iteration, or linear programming---yields both the optimal value function and the optimal policy. The two examples illustrate canonical MDP patterns: a binary-choice problem where high immediate reward aligns with favorable transitions, and a maintenance problem where the optimal policy balances operating revenue against replacement cost and failure risk.

\subsubsection{Key Equations}

Value of state $s_i$ under policy $\pi$:
\begin{equation*}
  V^\pi(s_i) = \mathbb{E}\!\left[\sum_{t=0}^{\infty} \gamma^t\, r(X_t, \pi(X_t)) \;\middle|\; X_0 = s_i\right].
\end{equation*}

Bellman equation (optimal value function):
\begin{equation*}
  V^*(s_i) = \max_{a \in \mathcal{A}} \left[ r(s_i, a) + \gamma \sum_{j=1}^{n} p_{ij}(a)\, V^*(s_j) \right].
\end{equation*}

Bellman's original finite-horizon recurrence:
\begin{equation*}
  f_N(s_i) = \max_{a} \left[ b(s_i, a) + \sum_{j=1}^{n} a_{ij}(a)\, f_{N-1}(s_j) \right].
\end{equation*}

Optimal policy:
\begin{equation*}
  \pi^*(s_i) = \arg\max_{a \in \mathcal{A}} \left[ r(s_i, a) + \gamma \sum_{j=1}^{n} p_{ij}(a)\, V^*(s_j) \right].
\end{equation*}

\subsubsection{Relevant Applications}
\begin{itemize}
  \item Inventory control and supply chain management, where the decision-maker chooses order quantities based on current stock levels to minimize holding and shortage costs.
  \item Financial portfolio management, where an investor rebalances assets over time to maximize risk-adjusted returns.
  \item Autonomous vehicle control, where the vehicle selects acceleration, braking, and steering actions based on its current state to optimize safety and efficiency.
  \item Game playing (e.g., board games, video games), where an agent selects moves to maximize a score or win probability.
  \item Energy systems and grid management, where operators decide on generation, storage, and dispatch to balance supply and demand under fluctuating conditions.
\end{itemize}

\section{Partially Observable Markov Decision Processes}
\label{sec:pomdp}

In the MDP formulation of Section~\ref{sec:mdp}, the agent observes the state directly at each time step. In many real-world problems this assumption is unrealistic: a robot may not know its exact position, a physician cannot observe a patient's internal state, and a maintenance operator may not know whether a machine component has degraded. A \textbf{partially observable Markov decision process} (POMDP) extends the MDP by introducing \emph{partial observability}: the agent cannot see the true state but instead receives noisy observations that depend probabilistically on it. This formulation was introduced by \AA str\"om \cite{astrom1965} and developed algorithmically by Smallwood and Sondik \cite{smallwood1973}.

\subsection{Notation}

\begin{tabular}{ll}
  \textbf{Symbol} & \textbf{Description} \\
  \hline
  $\Omega = \{o_1, \dots, o_k\}$ & Finite set of observations \\
  $O(o \mid s', a)$ & Observation probability: $P(\text{observe } o \mid \text{arrive in } s', \text{take } a)$ \\
  $\mathbf{b}$ & Belief state, a distribution over $\mathcal{S}$ \\
  $b(s_i)$ & Probability assigned to state $s_i$ in belief $\mathbf{b}$ \\
  $b'(s_j)$ & Updated belief after action $a$ and observation $o$ \\
  $\rho(\mathbf{b}, a)$ & Expected immediate reward under belief $\mathbf{b}$ and action $a$ \\
  $V^*(\mathbf{b})$ & Optimal value function over the belief simplex \\
  $P(o \mid \mathbf{b}, a)$ & Probability of observation $o$ given belief $\mathbf{b}$ and action $a$ \\
  $\mathbf{b}_{a,o}'$ & Updated belief after taking $a$ and observing $o$ \\
\end{tabular}

\subsection{Formal Definition}

A POMDP is defined by the tuple $(\mathcal{S}, \mathcal{A}, p, r, \Omega, O, \gamma)$:

\begin{itemize}
  \item $\mathcal{S}$, $\mathcal{A}$, $p_{ij}(a)$, $r(s_i, a)$, and $\gamma$: as in the MDP of Section~\ref{sec:mdp}.
  \item $\Omega = \{o_1, o_2, \dots, o_k\}$: a finite set of \textbf{observations} the agent may receive.
  \item $O(o \mid s', a)$: the \textbf{observation function}, giving the probability of receiving observation $o$ after taking action $a$ and arriving in state $s'$. For each $s'$ and $a$,
  \begin{equation}
    O(o \mid s', a) \geq 0 \quad \text{and} \quad \sum_{o \in \Omega} O(o \mid s', a) = 1.
  \end{equation}
\end{itemize}

When $|\Omega| = |\mathcal{S}|$ and $O(o_i \mid s_i, a) = 1$ for all $a$, each observation reveals the state exactly and the POMDP reduces to an MDP.

\subsection{Belief States}

Since the agent cannot observe the state directly, it maintains a \textbf{belief state} $\mathbf{b}$, a probability distribution over $\mathcal{S}$:
\begin{equation}
  \mathbf{b} = (b(s_1), b(s_2), \dots, b(s_n)), \quad b(s_i) \geq 0, \quad \sum_{i=1}^{n} b(s_i) = 1,
\end{equation}
where $b(s_i)$ is the agent's probability that the current state is $s_i$. After taking action $a$ and receiving observation $o$, the belief is updated via Bayes' rule:
\begin{equation} \label{eq:belief-update}
  b'(s_j) = \frac{O(o \mid s_j, a) \displaystyle\sum_{i=1}^{n} p_{ij}(a)\, b(s_i)}{\displaystyle\sum_{l=1}^{n} O(o \mid s_l, a) \sum_{i=1}^{n} p_{il}(a)\, b(s_i)}.
\end{equation}
The numerator first propagates the belief forward through the transition model (prediction), then weights by the observation likelihood (update). The denominator normalizes.

The belief state is a \emph{sufficient statistic}: it captures all information from the history of actions and observations that is relevant to future decisions.

\subsection{The Value Function and Bellman's Equation}

The key insight, due to \AA str\"om \cite{astrom1965}, is that a POMDP can be reformulated as a fully observable MDP over the \emph{belief space}---the $(n-1)$-dimensional probability simplex. In this \textbf{belief MDP}, the ``state'' is the belief $\mathbf{b}$, the expected immediate reward for taking action $a$ is
\begin{equation}
  \rho(\mathbf{b}, a) = \sum_{s \in \mathcal{S}} b(s)\, r(s, a),
\end{equation}
and the Bellman equation becomes
\begin{equation}
  V^*(\mathbf{b}) = \max_{a \in \mathcal{A}} \left[ \rho(\mathbf{b}, a) + \gamma \sum_{o \in \Omega} P(o \mid \mathbf{b}, a)\, V^*(\mathbf{b}_{a,o}') \right],
\end{equation}
where $P(o \mid \mathbf{b}, a) = \sum_{j} O(o \mid s_j, a) \sum_{i} p_{ij}(a)\, b(s_i)$ is the probability of observing $o$, and $\mathbf{b}_{a,o}'$ is the updated belief from (\ref{eq:belief-update}).

Unlike the MDP Bellman equation, which ranges over a finite set of states, this equation ranges over a \emph{continuous} belief space. For finite-horizon problems, Smallwood and Sondik \cite{smallwood1973} showed that $V^*$ is piecewise linear and convex over the belief simplex, which enables exact solution via alpha-vector methods. In general, however, exact finite-horizon POMDP planning is PSPACE-complete \cite{papadimitriou1987}, and practical algorithms rely on point-based approximations or online search.

\subsection{Example: The Tiger Problem}

The \textbf{Tiger Problem} \cite{kaelbling1998} is a canonical POMDP example. An agent faces two closed doors. Behind one door is a tiger (penalty $-100$); behind the other is a treasure (reward $+10$). The agent does not know which door hides the tiger but may \emph{listen} for a small cost ($-1$) to receive a noisy clue.

Formally: $\mathcal{S} = \{\text{TL}, \text{TR}\}$ (tiger left / tiger right), $\mathcal{A} = \{\text{listen}, \text{open-left}, \text{open-right}\}$, and $\Omega = \{\text{HL}, \text{HR}\}$ (hear left / hear right). Listening is $85\%$ accurate:
\begin{equation}
  O(\text{HL} \mid \text{TL}, \text{listen}) = 0.85, \quad O(\text{HR} \mid \text{TL}, \text{listen}) = 0.15,
\end{equation}
and symmetrically for state TR. Opening a door yields $+10$ (treasure) or $-100$ (tiger) and resets the problem with the tiger placed uniformly at random.

Rewards:
\begin{center}
\begin{tabular}{lcc}
  Action & Tiger behind chosen door & Tiger behind other door \\
  \hline
  Listen & $-1$ & $-1$ \\
  Open door & $-100$ & $+10$ \\
\end{tabular}
\end{center}

Starting from the uniform belief $b(\text{TL}) = 0.5$, the agent listens and hears the tiger on the left. Applying (\ref{eq:belief-update}):
\begin{equation}
  b'(\text{TL}) = \frac{0.85 \times 0.5}{0.85 \times 0.5 + 0.15 \times 0.5} = 0.85.
\end{equation}
After a second listen that again indicates left:
\begin{equation}
  b''(\text{TL}) = \frac{0.85 \times 0.85}{0.85 \times 0.85 + 0.15 \times 0.15} \approx 0.97.
\end{equation}
Now sufficiently confident the tiger is on the left, the agent opens the right door.

The optimal policy has an intuitive structure: when the belief is near $0.5$ (high uncertainty), listen to gather information; once the belief crosses a threshold (high confidence), open the door away from the tiger. This illustrates a central theme of POMDPs---\emph{information has value}. The listen action has negative immediate reward but reduces uncertainty, enabling better future decisions. This tradeoff between gathering information and acting on it is the hallmark of decision-making under partial observability.

\subsection{Summary}

The POMDP extends the MDP to settings where the agent cannot observe the true state and must instead rely on noisy observations. The key insight is that the agent's belief---a probability distribution over states, updated via Bayes' rule---serves as a sufficient statistic, transforming the POMDP into a continuous-state MDP over the belief simplex. This belief MDP preserves the Bellman equation structure but over an uncountably infinite state space, making exact finite-horizon planning PSPACE-complete in general. The Tiger Problem illustrates the core tension: information-gathering actions (listening) have negative immediate reward but reduce uncertainty, enabling higher-value decisions later. This \emph{value of information} is the defining concept of partially observable decision-making.

\subsubsection{Key Equations}

Belief state update (Bayes' rule):
\begin{equation*}
  b'(s_j) = \frac{O(o \mid s_j, a) \displaystyle\sum_{i=1}^{n} p_{ij}(a)\, b(s_i)}{\displaystyle\sum_{l=1}^{n} O(o \mid s_l, a) \sum_{i=1}^{n} p_{il}(a)\, b(s_i)}.
\end{equation*}

Expected immediate reward under belief $\mathbf{b}$:
\begin{equation*}
  \rho(\mathbf{b}, a) = \sum_{s \in \mathcal{S}} b(s)\, r(s, a).
\end{equation*}

Bellman equation over the belief simplex:
\begin{equation*}
  V^*(\mathbf{b}) = \max_{a \in \mathcal{A}} \left[ \rho(\mathbf{b}, a) + \gamma \sum_{o \in \Omega} P(o \mid \mathbf{b}, a)\, V^*(\mathbf{b}_{a,o}') \right].
\end{equation*}

\subsubsection{Relevant Applications}
\begin{itemize}
  \item Robot navigation and localization, where the robot infers its position from noisy sensor readings (e.g., lidar, GPS) while planning a path.
  \item Medical diagnosis and treatment planning, where the physician cannot directly observe the patient's disease state and must order tests (observations) before deciding on treatment.
  \item Spoken dialogue systems, where the system maintains a belief over the user's intent based on noisy speech recognition output.
  \item Aircraft collision avoidance, where the positions and intents of nearby aircraft are uncertain and must be inferred from radar observations.
  \item Inventory management under demand uncertainty, where the true demand distribution is unobserved and must be estimated from sales data.
\end{itemize}

\section{Multi-Armed Bandit Problems}
\label{sec:mab}

The \textbf{multi-armed bandit} (MAB) problem isolates a fundamental challenge that appears throughout the preceding sections: the tradeoff between \emph{exploration} (gathering information) and \emph{exploitation} (acting on current knowledge). The name comes from slot machines, colloquially called ``one-armed bandits.'' An agent faces $K$ such machines, each with an unknown reward distribution, and must decide which arm to pull at each time step to maximize cumulative reward.

The MAB is a degenerate MDP with a single state: the only decision is which action (arm) to take, and there are no state transitions. It can also be viewed as a POMDP in which the hidden state is the vector of true arm means and each reward observation provides partial information about them. The Bayesian perspective on bandits---where the agent maintains a posterior over the unknown parameters---connects directly to the belief-state framework of Section~\ref{sec:pomdp}.

The problem was first studied by Thompson \cite{thompson1933} and formalized in its modern form by Robbins \cite{robbins1952}.

\subsection{Notation}

\begin{tabular}{ll}
  \textbf{Symbol} & \textbf{Description} \\
  \hline
  $K$ & Number of arms \\
  $\mu_i$ & True mean reward of arm $i$ \\
  $T$ & Time horizon (number of rounds) \\
  $I_t$ & Arm selected at round $t$ \\
  $X_{I_t, t}$ & Reward received at round $t$ \\
  $R_T$ & Cumulative regret over $T$ rounds \\
  $\mu^*$ & Mean of the best arm, $\max_i \mu_i$ \\
  $\Delta_i$ & Gap (suboptimality) of arm $i$, $\mu^* - \mu_i$ \\
  $T_i(T)$ & Number of times arm $i$ is pulled in $T$ rounds \\
  $\varepsilon$ & Exploration probability ($\varepsilon$-greedy) \\
  $\hat{\mu}_i$ & Empirical mean reward of arm $i$ \\
  $N_i$ & Number of pulls of arm $i$ \\
  $\theta_i$ & True success probability of Bernoulli arm $i$ \\
  $S_i$ & Number of observed successes on arm $i$ \\
  $F_i$ & Number of observed failures on arm $i$ \\
  $\tilde{\theta}_i$ & Posterior sample for arm $i$ (Thompson Sampling) \\
  $G_i$ & Gittins index of arm $i$ \\
  $G(\alpha, \beta, \gamma)$ & Gittins index for a $\text{Beta}(\alpha, \beta)$ arm with discount $\gamma$; \\
  & $\alpha, \beta$ here are Beta shape parameters, not as in Section~\ref{sec:mc} \\
  $\text{KL}(\nu_i, \nu^*)$ & Kullback--Leibler divergence, arm $i$'s distribution to optimal \\
\end{tabular}

\subsection{Formal Definition}

An agent faces $K$ arms. Each arm $i \in \{1, \dots, K\}$ generates i.i.d.\ rewards from an unknown distribution with mean $\mu_i$. At each round $t = 1, 2, \dots, T$, the agent selects an arm $I_t$ and receives reward $X_{I_t, t}$. The agent's goal is to maximize cumulative reward, or equivalently to minimize \textbf{cumulative regret}:
\begin{equation}
  R_T = T \mu^* - \sum_{t=1}^{T} \mathbb{E}[X_{I_t, t}] = \sum_{i:\, \mu_i < \mu^*} \Delta_i \cdot \mathbb{E}[T_i(T)],
\end{equation}
where $\mu^* = \max_i \mu_i$ is the mean of the best arm, $\Delta_i = \mu^* - \mu_i$ is the \textbf{gap} (suboptimality) of arm~$i$, and $T_i(T)$ is the number of times arm~$i$ is pulled in $T$ rounds. Regret measures how much reward is lost by not always pulling the best arm.

\subsection{Strategies}

\subsubsection{$\varepsilon$-Greedy}

The simplest bandit strategy is $\varepsilon$-greedy: with probability $1 - \varepsilon$, pull the arm with the highest empirical mean (exploit); with probability $\varepsilon$, pull an arm uniformly at random (explore). While intuitive, a fixed $\varepsilon$ yields linear regret because the agent continues exploring at rate $\varepsilon$ indefinitely, even after the best arm is identified. Decaying $\varepsilon_t \propto t^{-1/3}$ improves this to $O(T^{2/3})$, but this is still far worse than the $O(\ln T)$ achievable by the methods below.

\subsubsection{Upper Confidence Bound (UCB1)}

The UCB1 algorithm \cite{auer2002} embodies the principle of \emph{optimism in the face of uncertainty}. After pulling each arm once, at every subsequent round the agent selects
\begin{equation}
  I_t = \arg\max_{i \in \{1,\dots,K\}} \left[ \hat{\mu}_i + \sqrt{\frac{2 \ln t}{N_i}} \right],
\end{equation}
where $\hat{\mu}_i$ is the empirical mean reward of arm~$i$ and $N_i$ is the number of times it has been pulled. The first term exploits current estimates; the second is an \emph{exploration bonus} that inflates the index of under-sampled arms. Arms with fewer pulls have wider confidence intervals and thus receive higher indices, ensuring they are eventually tried.

\subsubsection{Thompson Sampling}

Thompson Sampling \cite{thompson1933} takes a Bayesian approach. The agent maintains a posterior distribution over each arm's mean and, at each round, \emph{samples} from these posteriors and pulls the arm whose sample is highest.

For Bernoulli arms with unknown success probability $\theta_i$, a natural prior is $\theta_i \sim \text{Beta}(1, 1)$ (uniform). After observing $S_i$ successes and $F_i$ failures on arm~$i$, the posterior is $\theta_i \sim \text{Beta}(S_i + 1, F_i + 1)$. The algorithm is:
\begin{enumerate}
  \item For each arm $i$, sample $\tilde{\theta}_i \sim \text{Beta}(S_i + 1, F_i + 1)$.
  \item Pull arm $I_t = \arg\max_i \tilde{\theta}_i$.
  \item Observe reward $r_t \in \{0, 1\}$ and update: if $r_t = 1$, set $S_{I_t} \leftarrow S_{I_t} + 1$; otherwise $F_{I_t} \leftarrow F_{I_t} + 1$.
\end{enumerate}
When the posterior is concentrated on a high mean, the samples are likely to be high (exploitation); when it is diffuse, samples are variable and the arm may be tried (exploration). The Beta--Bernoulli conjugacy ensures the posterior update is a simple counter increment.

\subsubsection{The Gittins Index}

For the discounted infinite-horizon Bayesian bandit (discount factor $\gamma$, known prior on each arm), Gittins \cite{gittins1979} proved a remarkable result: the optimal policy is an \emph{index policy}. Each arm~$i$ is assigned a \textbf{Gittins index} $G_i$ computed from its posterior alone, and the agent always pulls the arm with the highest index. The index can be interpreted as the fair price at which the agent would be indifferent between continuing to pull the arm and receiving a guaranteed constant payoff forever. The Gittins index theorem shows that the multi-arm problem decomposes into independent single-arm problems---a dramatic simplification. This optimality breaks down, however, when arms evolve even while not being pulled, motivating the restless bandit model of Section~\ref{sec:restless}.

\subsubsection{Successive Elimination}

Rather than selecting arms via an index, \textbf{successive elimination} \cite{evendar2006} maintains a set of \emph{active} arms and permanently discards any arm whose upper confidence bound falls below another arm's lower confidence bound. Once eliminated, an arm is never pulled again. The algorithm achieves $O(K \ln T)$ regret while offering a conceptually different mechanism: instead of balancing exploration and exploitation continuously, it reduces the problem size over time.

\subsubsection{KL-UCB}

\textbf{KL-UCB} \cite{garivier2011} replaces the Hoeffding-based confidence bound in UCB1 with a tighter bound derived from the Kullback--Leibler divergence. At each round the agent selects $I_t = \arg\max_i \sup\{q : N_i \cdot \text{KL}(\hat{\mu}_i, q) \leq \ln t\}$, where $\text{KL}(\hat{\mu}_i, q)$ is the KL divergence between Bernoulli distributions with parameters $\hat{\mu}_i$ and $q$. The resulting bound is asymptotically optimal, matching the Lai--Robbins lower bound with the exact constant---an improvement over UCB1, which matches only up to a multiplicative factor.

\subsubsection{EXP3}

All strategies above assume \emph{stochastic} rewards (i.i.d.\ draws from fixed distributions). \textbf{EXP3} (Exponential-weight algorithm for Exploration and Exploitation) \cite{auer2002nonstochastic} handles the \emph{adversarial} setting, where rewards may be chosen by an adversary with no distributional assumption. The algorithm maintains a probability distribution over arms, updated via exponential weights on estimated rewards, and achieves $O(\sqrt{KT \ln K})$ regret against the best fixed arm in hindsight.

\subsection{Regret Bounds}

Lai and Robbins \cite{lai1985} established a fundamental \textbf{lower bound} on regret. For any consistent policy\footnote{A policy is \emph{consistent} if $\mathbb{E}[T_i(T)] = o(T^\alpha)$ for every suboptimal arm~$i$ and every $\alpha > 0$.},
\begin{equation}
  \liminf_{T \to \infty} \frac{\mathbb{E}[R_T]}{\ln T} \geq \sum_{i:\, \mu_i < \mu^*} \frac{\Delta_i}{\text{KL}(\nu_i, \nu^*)},
\end{equation}
where $\text{KL}(\nu_i, \nu^*)$ is the Kullback--Leibler divergence from arm~$i$'s distribution to the optimal arm's distribution. This says regret must grow at least logarithmically in $T$, and distinguishing a suboptimal arm from the best requires more pulls when their distributions are similar (small KL divergence).

UCB1 achieves a finite-time upper bound of
\begin{equation}
  \mathbb{E}[R_T] \leq \sum_{i:\, \mu_i < \mu^*} \frac{8 \ln T}{\Delta_i} + \left(1 + \frac{\pi^2}{3}\right) \sum_i \Delta_i,
\end{equation}
which is $O(K \ln T)$---logarithmic in $T$ and matching the Lai--Robbins lower bound up to constant factors for bounded rewards. Thompson Sampling also achieves asymptotically optimal regret under standard regularity conditions on the prior \cite{auer2002}.

\subsection{Examples}

We illustrate each strategy on a common instance: $K = 3$ arms with Bernoulli success probabilities $\theta_1 = 0.3$, $\theta_2 = 0.7$, and $\theta_3 = 0.5$. Arm~2 is optimal; the agent does not know the $\theta_i$ and must learn from rewards.

\subsubsection{$\varepsilon$-Greedy}

Set $\varepsilon = 0.3$. After initializing by pulling each arm once, the agent exploits (pulls the arm with the highest empirical mean) with probability $0.7$ and explores (pulls a uniformly random arm) with probability $0.3$.

\begin{center}
\begin{tabular}{cclcl}
  $t$ & Arm & Type & Reward & $\hat{\mu} = (\hat{\mu}_1, \hat{\mu}_2, \hat{\mu}_3)$ \\
  \hline
  1 & 1 & init & 0 & $(0.00,\; {-},\; {-})$ \\
  2 & 2 & init & 1 & $(0.00,\; 1.00,\; {-})$ \\
  3 & 3 & init & 1 & $(0.00,\; 1.00,\; 1.00)$ \\
  4 & 2 & exploit & 1 & $(0.00,\; 1.00,\; 1.00)$ \\
  5 & 2 & exploit & 0 & $(0.00,\; 0.67,\; 1.00)$ \\
  6 & 3 & exploit & 0 & $(0.00,\; 0.67,\; 0.50)$ \\
  7 & 2 & exploit & 1 & $(0.00,\; 0.75,\; 0.50)$ \\
  8 & 1 & \textbf{explore} & 0 & $(0.00,\; 0.75,\; 0.50)$ \\
  9 & 2 & exploit & 1 & $(0.00,\; 0.80,\; 0.50)$ \\
  10 & 3 & \textbf{explore} & 0 & $(0.00,\; 0.80,\; 0.33)$ \\
\end{tabular}
\end{center}

After ten rounds the pull counts are $N = (2, 5, 3)$. Arm~2 has been pulled most often, and its empirical mean ($0.80$) is close to the true $\theta_2 = 0.7$. But notice the two explore rounds ($t = 8, 10$): at $t = 8$, the agent wastes a pull on arm~1 despite strong evidence it is the worst arm. This is the fundamental limitation of $\varepsilon$-greedy---it explores \emph{uniformly at random}, directing equal attention to clearly bad and merely uncertain arms.

\subsubsection{UCB1}

We trace UCB1 through seven rounds on the same instance.

\textbf{Rounds 1--3 (initialization):} Pull each arm once.
\begin{center}
\begin{tabular}{ccccl}
  $t$ & Arm & Reward & $\hat{\mu}_i$ & \\
  \hline
  1 & 1 & 1 & $(1.00,\; {-},\; {-})$ & \\
  2 & 2 & 1 & $(1.00,\; 1.00,\; {-})$ & \\
  3 & 3 & 0 & $(1.00,\; 1.00,\; 0.00)$ & \\
\end{tabular}
\end{center}

\textbf{Rounds 4--7 (UCB1 selection):} At each round, compute the index $\hat{\mu}_i + \sqrt{2 \ln t / N_i}$ and pull the arm with the highest value.

\begin{center}
\begin{tabular}{cccccl}
  $t$ & UCB$_1$ & UCB$_2$ & UCB$_3$ & Arm & Reward \\
  \hline
  4 & $\mathbf{2.67}$ & $2.67$ & $1.67$ & 1 & 0 \\
  5 & $1.77$ & $\mathbf{2.79}$ & $1.79$ & 2 & 1 \\
  6 & $1.84$ & $\mathbf{2.34}$ & $1.89$ & 2 & 0 \\
  7 & $1.90$ & $1.81$ & $\mathbf{1.97}$ & 3 & 1 \\
\end{tabular}
\end{center}

After seven rounds the pull counts are $N = (2, 3, 2)$ and the empirical means are $\hat{\mu} = (0.50, 0.67, 0.50)$. Already arm~2 (the true best) has been pulled most often, while the exploration bonus ensures that arm~3 is revisited at $t = 7$ when its uncertainty grows large enough. Unlike $\varepsilon$-greedy, UCB1 directs exploration toward arms whose \emph{uncertainty} is high, not at random.

\subsubsection{Thompson Sampling}

Each arm begins with a uniform prior $\theta_i \sim \text{Beta}(1, 1)$. At each round, the agent samples from the current posteriors and pulls the arm with the highest sample.

\begin{center}
\begin{tabular}{ccccccl}
  $t$ & $\tilde{\theta}_1$ & $\tilde{\theta}_2$ & $\tilde{\theta}_3$ & Arm & Reward & Posteriors \\
  \hline
  1 & 0.42 & $\mathbf{0.88}$ & 0.57 & 2 & 1 & $\text{B}(1,1),\; \text{B}(2,1),\; \text{B}(1,1)$ \\
  2 & 0.71 & 0.60 & $\mathbf{0.82}$ & 3 & 0 & $\text{B}(1,1),\; \text{B}(2,1),\; \text{B}(1,2)$ \\
  3 & 0.65 & $\mathbf{0.94}$ & 0.28 & 2 & 1 & $\text{B}(1,1),\; \text{B}(3,1),\; \text{B}(1,2)$ \\
  4 & $\mathbf{0.85}$ & 0.78 & 0.14 & 1 & 0 & $\text{B}(1,2),\; \text{B}(3,1),\; \text{B}(1,2)$ \\
  5 & 0.18 & $\mathbf{0.72}$ & 0.53 & 2 & 1 & $\text{B}(1,2),\; \text{B}(4,1),\; \text{B}(1,2)$ \\
  6 & 0.33 & $\mathbf{0.91}$ & 0.27 & 2 & 0 & $\text{B}(1,2),\; \text{B}(4,2),\; \text{B}(1,2)$ \\
  7 & 0.09 & $\mathbf{0.67}$ & 0.42 & 2 & 1 & $\text{B}(1,2),\; \text{B}(5,2),\; \text{B}(1,2)$ \\
\end{tabular}
\end{center}

After seven rounds the pull counts are $N = (1, 5, 1)$. The posterior for arm~2 is $\text{Beta}(5, 2)$, with mean $5/7 \approx 0.71$---close to the true $\theta_2 = 0.7$---while the posteriors for arms~1 and~3 remain diffuse. At $t = 4$, arm~1's diffuse posterior $\text{Beta}(1, 1)$ happens to produce a high sample ($0.85$), triggering an exploration pull; the resulting failure shifts the posterior to $\text{Beta}(1, 2)$, making future high samples less likely. This illustrates how Thompson Sampling \emph{self-corrects}: it explores in proportion to the posterior probability that each arm is optimal, naturally reducing exploration of inferior arms as evidence accumulates.

\subsubsection{The Gittins Index}

The Gittins index is defined for the discounted Bayesian setting and requires computing the value of optimally playing a single arm against a known alternative---a calculation that does not reduce to a simple round-by-round trace. For a Beta--Bernoulli arm with posterior $\text{Beta}(\alpha, \beta)$ (where $\alpha, \beta$ are the shape parameters of the Beta distribution, not the transition probabilities of Section~\ref{sec:mc}) and discount factor $\gamma$, the index $G(\alpha, \beta, \gamma)$ can be computed numerically via dynamic programming or calibration. Tables of Gittins indices show, for example, that $G(1, 1, 0.9) \approx 0.7025$, meaning an arm with a uniform prior is worth playing as long as its ``retirement reward'' is below roughly $0.70$. As successes accumulate the index rises (e.g., $G(5, 2, 0.9) \approx 0.7867$); as failures accumulate it falls (e.g., $G(1, 2, 0.9) \approx 0.5000$). The agent always plays the arm with the highest current index, achieving the Bayes-optimal policy without ever computing a joint posterior over all arms.

\subsection{Summary}

The multi-armed bandit distills the exploration--exploitation tradeoff into its purest form: a single-state decision problem where the agent must learn which action is best by trying them. The $\varepsilon$-greedy strategy offers simplicity but wastes exploration on clearly inferior arms; UCB1 directs exploration toward uncertain arms using confidence bounds; Thompson Sampling leverages Bayesian posteriors to explore in proportion to each arm's probability of being optimal; and the Gittins index provides the Bayes-optimal solution for the discounted infinite-horizon case by reducing the multi-arm problem to independent single-arm computations. The Lai--Robbins lower bound establishes that logarithmic regret growth is the best any consistent policy can achieve, and both UCB1 and Thompson Sampling attain this rate. Extensions such as KL-UCB sharpen the constants, successive elimination reduces the active arm set over time, and EXP3 handles adversarial (non-stochastic) environments.

\subsubsection{Key Equations}

Cumulative regret:
\begin{equation*}
  R_T = T \mu^* - \sum_{t=1}^{T} \mathbb{E}[X_{I_t, t}] = \sum_{i:\, \mu_i < \mu^*} \Delta_i \cdot \mathbb{E}[T_i(T)].
\end{equation*}

UCB1 arm selection:
\begin{equation*}
  I_t = \arg\max_{i \in \{1,\dots,K\}} \left[ \hat{\mu}_i + \sqrt{\frac{2 \ln t}{N_i}} \right].
\end{equation*}

Lai--Robbins lower bound:
\begin{equation*}
  \liminf_{T \to \infty} \frac{\mathbb{E}[R_T]}{\ln T} \geq \sum_{i:\, \mu_i < \mu^*} \frac{\Delta_i}{\text{KL}(\nu_i, \nu^*)}.
\end{equation*}

UCB1 upper bound:
\begin{equation*}
  \mathbb{E}[R_T] \leq \sum_{i:\, \mu_i < \mu^*} \frac{8 \ln T}{\Delta_i} + \left(1 + \frac{\pi^2}{3}\right) \sum_i \Delta_i.
\end{equation*}

Thompson Sampling posterior update (Bernoulli arms):
\begin{equation*}
  \theta_i \mid \text{data} \sim \text{Beta}(S_i + 1,\; F_i + 1).
\end{equation*}

\subsubsection{Relevant Applications}
\begin{itemize}
  \item Clinical trial design, where treatments (arms) must be allocated to patients to identify the most effective therapy while minimizing harm.
  \item Online advertising and recommendation systems, where the platform must balance showing proven content against testing new alternatives.
  \item A/B testing and website optimization, where variants of a webpage or feature are compared in real time.
  \item Dynamic pricing, where a seller experiments with price points to learn demand curves while maximizing revenue.
  \item Network routing and server selection, where traffic must be allocated across paths or servers with unknown performance characteristics.
\end{itemize}

\section{Restless Bandit Problems}
\label{sec:restless}

The classical multi-armed bandit of Section~\ref{sec:mab} assumes that \emph{unplayed arms are frozen}: an arm's state changes only when it is pulled. In many applications this is unrealistic---machines degrade whether or not they are serviced, communication channels fluctuate whether or not they are probed, and patients' health evolves whether or not they receive treatment. The \textbf{restless bandit} model, introduced by Whittle \cite{whittle1988}, removes this restriction: every arm transitions at every time step, regardless of whether it is played. This seemingly modest generalization has profound consequences for tractability and policy design.

\subsection{Notation}

\begin{tabular}{ll}
  \textbf{Symbol} & \textbf{Description} \\
  \hline
  $\mathcal{S}_i$ & State space of arm $i$ \\
  $p_{ij}^{(1)}$ & Transition probability under the active action \\
  $p_{ij}^{(0)}$ & Transition probability under the passive action \\
  $r_i(s)$ & Reward of arm $i$ in state $s$ \\
  $M$ & Number of arms activated per time step \\
  $a_i^t$ & Activation indicator for arm $i$ at time $t$ \\
  $\mathbf{P}^{(0)}$ & Passive transition matrix \\
  $\mathbf{P}^{(1)}$ & Active transition matrix \\
  $\mathbf{I}$ & Identity matrix \\
  $\lambda$ & Lagrange multiplier (subsidy for passivity) \\
  $W(s)$ & Whittle index of state $s$ \\
  $V^{\text{Whittle}}(\mathbf{s})$ & Value under the Whittle index policy at joint state $\mathbf{s}$ \\
\end{tabular}

\subsection{Formal Definition}

A restless bandit consists of $K$ independent arms, each modeled as a two-action MDP. Arm~$i$ has a finite state space $\mathcal{S}_i$ and two actions: \emph{active} (play the arm) and \emph{passive} (leave it alone). The transition dynamics depend on the action taken:
\begin{align}
  \text{active:}  &\quad p_{ij}^{(1)} = P(s' = j \mid s = i, \text{active}), \\
  \text{passive:} &\quad p_{ij}^{(0)} = P(s' = j \mid s = i, \text{passive}).
\end{align}
The reward $r_i(s)$ depends on the state of arm~$i$.\footnote{More generally, the reward may also depend on the action: $r_i(s, a)$. We use $r_i(s)$ here for simplicity, as in many applications the reward reflects the arm's state rather than the chosen action.} At each time step, the agent must activate exactly $M$ of the $K$ arms (with $M < K$); the remaining $K - M$ arms evolve passively. The objective is to maximize the expected total discounted reward:
\begin{equation}
  \max \; \mathbb{E}\!\left[\sum_{t=0}^{\infty} \gamma^t \sum_{i=1}^{K} r_i(s_i^t)\right] \quad \text{subject to} \quad \sum_{i=1}^{K} a_i^t = M \;\text{ for all } t,
\end{equation}
where $a_i^t \in \{0, 1\}$ indicates whether arm~$i$ is active at time~$t$.

The classical (``rested'') bandit of Section~\ref{sec:mab} is the special case where passive arms are frozen: $\mathbf{P}^{(0)} = \mathbf{I}$. In the restless model, $\mathbf{P}^{(0)} \neq \mathbf{I}$, so all arms evolve simultaneously.

\subsection{Intractability}

The joint state space of a $K$-arm restless bandit is the product $\mathcal{S}_1 \times \cdots \times \mathcal{S}_K$, which grows exponentially in $K$. Papadimitriou and Tsitsiklis \cite{papadimitriou1999} proved that the restless bandit problem is \textbf{PSPACE-hard}---a strictly stronger form of intractability than NP-hardness. Even computing a nontrivially good approximate policy is PSPACE-hard in general. This motivates the use of index heuristics.

\subsection{Whittle's Relaxation and the Whittle Index}

Whittle \cite{whittle1988} proposed a Lagrangian relaxation that replaces the hard constraint (activate exactly $M$ arms at every step) with a time-average constraint:
\begin{equation}
  (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t \sum_{i=1}^{K} \mathbb{E}[a_i^t] \leq M.
\end{equation}
Introducing a Lagrange multiplier $\lambda$---interpreted as a \emph{subsidy for passivity}---the relaxed objective becomes
\begin{equation}
  \max \; \mathbb{E}\!\left[\sum_{t=0}^{\infty} \gamma^t \sum_{i=1}^{K} \big(r_i(s_i^t) + \lambda\,(1 - a_i^t)\big)\right].
\end{equation}
This decomposes into $K$ independent single-arm subproblems: for each arm, solve a two-action MDP with an additional reward of $\lambda$ for choosing the passive action. Each subproblem is a standard MDP and can be solved in polynomial time.

An arm is said to be \textbf{indexable} if the set of states in which the passive action is optimal grows monotonically as $\lambda$ increases---that is, as passivity becomes more lucrative, the agent becomes passive in progressively more states. When indexability holds, the \textbf{Whittle index} of a state $s$ is the critical subsidy
\begin{equation}
  W(s) = \inf\!\big\{\lambda : \text{passive is optimal in state } s \text{ under subsidy } \lambda\big\}.
\end{equation}
Intuitively, $W(s)$ measures the \emph{value of activating} an arm in state $s$: it is the minimum per-period payment required to make the agent willing to leave that arm idle.

The \textbf{Whittle index policy} is then: at each time step, activate the $M$ arms with the highest Whittle indices. This policy is a heuristic---it is not guaranteed to be optimal---but Weber and Weiss \cite{weber1990} proved that it is \textbf{asymptotically optimal} as $K \to \infty$ (with $M/K$ held constant), provided the arms are indexable and a technical stability condition holds.

Not all restless bandits are indexable. When indexability fails, the Whittle index is not well-defined and alternative approaches (LP relaxations, reinforcement learning) must be used. Verifying indexability for a given arm requires checking that its optimal policy has a threshold structure in $\lambda$.

\subsection{Example: Machine Maintenance}

Consider $K = 3$ identical machines and $M = 1$ repairman. Each machine has states $\mathcal{S} = \{\text{G (Good)}, \text{B (Bad)}\}$ with rewards $r(\text{G}) = 1$ and $r(\text{B}) = 0$. The transition matrices are:
\begin{equation}
  \mathbf{P}^{(0)} = \begin{pmatrix} 0.9 & 0.1 \\ 0.2 & 0.8 \end{pmatrix}, \qquad
  \mathbf{P}^{(1)} = \begin{pmatrix} 0.95 & 0.05 \\ 0.7 & 0.3 \end{pmatrix},
\end{equation}
where rows correspond to the current state (G, B) and columns to the next state. Under passive operation, a Good machine degrades with probability $0.1$ per period; a Bad machine self-repairs with probability $0.2$. Active maintenance (the repairman) reduces degradation and greatly improves repair: a Bad machine returns to Good with probability $0.7$ instead of $0.2$.

The discount factor is $\gamma = 0.9$. Computing the Whittle indices by binary search on $\lambda$ yields:
\begin{equation}
  W(\text{G}) \approx 0.058, \qquad W(\text{B}) \approx 1.216.
\end{equation}
The large gap $W(\text{B}) \gg W(\text{G})$ reflects an intuitive priority: a broken machine benefits far more from the repairman than a functioning one.

The Whittle index policy assigns the repairman to the machine with the highest index---always a Bad machine when one exists. We trace five steps starting from states $(\text{G}, \text{B}, \text{B})$:

\begin{center}
\begin{tabular}{cccccc}
  $t$ & States & $W$ & Activate & Transitions & Reward \\
  \hline
  0 & (G, B, B) & $(0.06,\; \mathbf{1.22},\; 1.22)$ & M2 & M2: B$\to$G & 1 \\
  1 & (G, G, B) & $(0.06,\; 0.06,\; \mathbf{1.22})$ & M3 & M3: B$\to$G & 2 \\
  2 & (G, G, G) & $(0.06,\; 0.06,\; 0.06)$ & M1 & M2: G$\to$B & 3 \\
  3 & (G, B, G) & $(0.06,\; \mathbf{1.22},\; 0.06)$ & M2 & M3: G$\to$B & 2 \\
  4 & (G, G, B) & $(0.06,\; 0.06,\; \mathbf{1.22})$ & M3 & M3: B$\to$G & 2 \\
\end{tabular}
\end{center}

Several features of restless bandits are visible in this trace. At $t = 0$, machines~2 and~3 are both Bad; the repairman services machine~2 (tie-breaking), and it returns to Good. Meanwhile machine~3 remains passive and stays Bad---it does not freeze as it would in a classical bandit. At $t = 2$, all machines are Good and the indices are equal; the repairman performs preventive maintenance on machine~1, but machine~2 degrades (G$\to$B with passive probability $0.1$) despite being unattended. This \emph{degradation while idle} is the defining feature of the restless model.

For this small instance, the Whittle index policy can be verified against the brute-force optimum over all $2^3 = 8$ joint states. The two policies differ at three states (e.g., in state $(\text{B}, \text{B}, \text{B})$, the optimal policy may activate machine~3 while the Whittle policy activates machine~1), but because the machines are identical these tie-breaking differences have no effect on value: $V^*(\mathbf{s}) = V^{\text{Whittle}}(\mathbf{s})$ for all $\mathbf{s}$.

The following Python code computes the Whittle indices and solves the exact problem via value iteration:

\begin{lstlisting}[caption={Whittle index computation and verification for the machine maintenance restless bandit.}]
import numpy as np
from itertools import product

gamma = 0.9
P0 = np.array([[0.9, 0.1], [0.2, 0.8]])   # passive
P1 = np.array([[0.95, 0.05], [0.7, 0.3]])  # active
r = np.array([1.0, 0.0])                    # r(G)=1, r(B)=0

# --- Whittle index via binary search on subsidy lambda ---
for state in [0, 1]:  # 0=Good, 1=Bad
    lo, hi = -10.0, 10.0
    for _ in range(200):
        lam = (lo + hi) / 2.0
        V = np.zeros(2)
        for _ in range(100000):
            Qp = r + lam + gamma * P0 @ V
            Qa = r + gamma * P1 @ V
            V_new = np.maximum(Qp, Qa)
            if np.max(np.abs(V_new - V)) < 1e-12:
                break
            V = V_new
        if Qp[state] > Qa[state]:
            hi = lam
        else:
            lo = lam
    name = "Good" if state == 0 else "Bad"
    print(f"W({name}) = {(lo+hi)/2:.4f}")

# --- Brute-force optimal for K=3, M=1 ---
states = list(product([0, 1], repeat=3))
V = np.zeros(8)
for _ in range(100000):
    V_new = np.zeros(8)
    for si, s in enumerate(states):
        best = -float("inf")
        for a in range(3):  # activate machine a
            rew = sum(r[s[i]] for i in range(3))
            P = [P1 if i == a else P0 for i in range(3)]
            val = rew + gamma * sum(
                np.prod([P[i][s[i]][sn[i]] for i in range(3)])
                * V[sn[0]*4 + sn[1]*2 + sn[2]]
                for sn in product([0,1], repeat=3))
            best = max(best, val)
        V_new[si] = best
    if np.max(np.abs(V_new - V)) < 1e-12:
        break
    V = V_new
for si, s in enumerate(states):
    names = ["G" if x == 0 else "B" for x in s]
    print(f"V*({','.join(names)}) = {V[si]:.2f}")
\end{lstlisting}

\subsection{Summary}

The restless bandit generalizes the classical multi-armed bandit by allowing all arms to evolve at every time step, whether or not they are played. This removes the ``frozen when idle'' assumption of the Gittins model and greatly increases the problem's complexity---from polynomial-time solvable (via the Gittins index) to PSPACE-hard. Whittle's Lagrangian relaxation partially restores tractability by decomposing the coupled problem into independent single-arm subproblems, yielding the Whittle index policy. This heuristic is asymptotically optimal under indexability and stability conditions, and performs well in practice across a range of domains.

\subsubsection{Key Equations}

Restless arm transition dynamics (active and passive):
\begin{equation*}
  p_{ij}^{(1)} = P(s' = j \mid s = i, \text{active}), \qquad p_{ij}^{(0)} = P(s' = j \mid s = i, \text{passive}).
\end{equation*}

Whittle's Lagrangian relaxation (time-average activation constraint):
\begin{equation*}
  (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t \sum_{i=1}^{K} \mathbb{E}[a_i^t] \leq M.
\end{equation*}

Relaxed objective with subsidy for passivity:
\begin{equation*}
  \max \; \mathbb{E}\!\left[\sum_{t=0}^{\infty} \gamma^t \sum_{i=1}^{K} \big(r_i(s_i^t) + \lambda\,(1 - a_i^t)\big)\right].
\end{equation*}

Whittle index (critical subsidy):
\begin{equation*}
  W(s) = \inf\!\big\{\lambda : \text{passive is optimal in state } s \text{ under subsidy } \lambda\big\}.
\end{equation*}

\subsubsection{Relevant Applications}
\begin{itemize}
  \item Preventive maintenance scheduling for fleets of machines or infrastructure assets that degrade whether or not they are serviced.
  \item Wireless channel scheduling, where channel quality fluctuates due to fading and interference regardless of whether a channel is used.
  \item Healthcare intervention programs (e.g., maternal health, chronic disease management), where patient adherence and health status evolve between contacts.
  \item Sensor management and target tracking, where targets move and hide whether or not they are under surveillance.
  \item Web crawling and content caching, where page freshness decays whether or not pages are re-fetched.
\end{itemize}

\section{Beyond the Scope of This Paper}
\label{sec:beyond}

The preceding sections trace a single thread through the landscape of Markov models: passive chains (Section~\ref{sec:mc}), controlled chains with full observability (Section~\ref{sec:mdp}), controlled chains with partial observability (Section~\ref{sec:pomdp}), and the exploration--exploitation specializations that arise when state structure is absent or arms evolve independently (Sections~\ref{sec:mab}--\ref{sec:restless}). This thread is coherent but not exhaustive. Several major model classes occupy adjacent or intersecting regions of the same landscape and are briefly noted here.

\subsection{Hidden Markov Models}

A \textbf{hidden Markov model} (HMM) \cite{rabiner1989} is a Markov chain whose states are unobservable; at each time step the system emits an observation drawn from a state-dependent distribution. The HMM is the passive, uncontrolled counterpart of the POMDP: it shares the hidden-state and observation machinery of Section~\ref{sec:pomdp} but has no actions or rewards. This completes a natural $2 \times 2$ classification---\{observable, hidden\} $\times$ \{passive, controlled\} = \{Markov chain, HMM, MDP, POMDP\}. The central computational problems for HMMs---filtering (the forward algorithm), smoothing (forward--backward), and parameter estimation (Baum--Welch / EM)---underpin speech recognition, gene finding, and financial time-series analysis.

\textbf{Applications:}
\begin{itemize}
  \item Speech recognition, where hidden phoneme sequences generate observed acoustic signals.
  \item Gene finding and biological sequence analysis, where hidden functional regions emit nucleotide patterns.
  \item Financial regime detection, where latent market states (bull, bear, crisis) drive observable returns.
  \item Part-of-speech tagging and natural language processing, where hidden grammatical roles produce observed words.
  \item Activity recognition from sensor data, where unobserved human activities generate accelerometer and gyroscope readings.
\end{itemize}

\subsection{Contextual Bandits}

The \textbf{contextual bandit} \cite{langford2008} extends the multi-armed bandit of Section~\ref{sec:mab} by providing side information---a \emph{context} vector---before each decision. The reward distribution of each arm depends on the context, so the agent must learn a mapping from contexts to optimal arms rather than a single best arm. Contextual bandits sit between the stateless MAB and the full MDP: they introduce state (the context) but assume no state transitions---contexts arrive exogenously and do not depend on past actions. This model is the dominant formulation in modern recommendation systems, personalized medicine, and online advertising, where user features constitute the context and content or treatment options constitute the arms.

\textbf{Applications:}
\begin{itemize}
  \item News article and content recommendation, where user profile is the context and articles are arms.
  \item Personalized medicine and adaptive clinical trials, where patient features guide treatment selection.
  \item Online advertising and ad placement, where user demographics and browsing history inform bid strategies.
  \item Dynamic pricing, where product features and market conditions determine optimal price points.
  \item Dialog systems and chatbot response selection conditioned on conversation history.
\end{itemize}

\subsection{Markov Games}

All models in Sections~\ref{sec:mc}--\ref{sec:restless} involve a single decision-maker. A \textbf{Markov game} (or stochastic game) \cite{shapley1953} generalizes the MDP to multiple agents who act simultaneously, each with their own reward function. Transition probabilities and rewards depend on the \emph{joint} action of all agents. The solution concept shifts from an optimal policy to a Nash equilibrium---a profile of policies from which no agent can unilaterally improve. Special cases include zero-sum games (purely competitive) and team games (fully cooperative). When agents also have private observations, the model becomes a \textbf{decentralized POMDP} (Dec-POMDP) \cite{bernstein2002}, which is NEXP-complete---harder even than single-agent POMDPs.

\textbf{Applications:}
\begin{itemize}
  \item Multi-player games and competitive AI (poker, real-time strategy, autonomous racing).
  \item Multi-robot coordination for search-and-rescue, warehouse logistics, or formation control.
  \item Economic market modeling, where firms or agents interact strategically over time.
  \item Cybersecurity, where attacker and defender adapt strategies in response to each other.
  \item Multi-agent traffic signal control in urban networks.
\end{itemize}

\subsection{Constrained MDPs}

The MDP of Section~\ref{sec:mdp} optimizes a single scalar objective. A \textbf{constrained MDP} (CMDP) \cite{altman1999} adds side constraints of the form $\mathbb{E}[\sum_t \gamma^t c_k(s_t, a_t)] \leq C_k$, requiring that expected cumulative costs remain within budgets. The optimal policy is generally a \emph{mixture} of deterministic policies, computable via linear programming. CMDPs are increasingly important in safety-critical applications---autonomous driving, healthcare, robotics---where constraint satisfaction (e.g., collision probability, dosage limits) is as important as reward maximization.

\textbf{Applications:}
\begin{itemize}
  \item Autonomous driving with safety constraints on collision probability and lane departure.
  \item Clinical trial design with dosage limits and adverse-event budgets.
  \item Power grid management subject to emission caps and reliability standards.
  \item Portfolio optimization with drawdown or value-at-risk constraints.
  \item Wireless resource allocation under fairness or quality-of-service guarantees.
\end{itemize}

\subsection{Semi-Markov Decision Processes}

The MDP assumes that every action takes exactly one time step. A \textbf{semi-Markov decision process} (SMDP) \cite{puterman1994} relaxes this by allowing actions to have variable, random durations (sojourn times). Rewards accrue over the holding period, and discounting accounts for elapsed time rather than step count. SMDPs are natural models for manufacturing, maintenance scheduling, and queueing, where the time between decisions varies. They also provide the formal foundation for the \emph{options} framework in hierarchical reinforcement learning, where temporally extended actions (options) execute over multiple primitive steps.

\textbf{Applications:}
\begin{itemize}
  \item Maintenance scheduling for industrial equipment, where repair and inspection durations vary.
  \item Queueing systems and call-center staffing, where service times are random.
  \item Inventory management with stochastic lead times for replenishment.
  \item Hierarchical task planning in robotics, where high-level skills (options) span variable numbers of primitive actions.
  \item Telecommunications network routing, where packet travel times fluctuate with congestion.
\end{itemize}

\subsection{Model-Free Reinforcement Learning}

The solution methods in this paper---value iteration, policy evaluation, Bellman equations solved in closed form---all assume the transition probabilities and rewards are known. \textbf{Model-free reinforcement learning} \cite{sutton2018} addresses the setting where the agent must learn to act \emph{without} a model, using only sampled experience. Algorithms such as Q-learning, SARSA, and policy gradient methods (REINFORCE, PPO, actor-critic) learn value functions or policies directly from interaction. Deep reinforcement learning combines these algorithms with neural network function approximation, enabling application to problems with enormous state spaces---game playing, robotic manipulation, chip design---where tabular dynamic programming is infeasible. Model-free RL is not a model class in the same sense as the others listed here; it is a solution methodology that applies across MDPs, POMDPs, and multi-agent settings.

\textbf{Applications:}
\begin{itemize}
  \item Game playing (Atari, Go, chess, poker) where the state space is too large for tabular methods.
  \item Robotic locomotion and manipulation, where policies are learned from simulated or real-world interaction.
  \item Autonomous vehicle control, including lane keeping, merging, and racing.
  \item Chip placement and hardware design optimization.
  \item Large language model alignment via reinforcement learning from human feedback (RLHF).
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

This paper has traced a single coherent thread through the theory of sequential decision-making under uncertainty: from the passive Markov chain (Section~\ref{sec:mc}), where a system evolves according to fixed transition probabilities, to the Markov decision process (Section~\ref{sec:mdp}), where an agent's actions influence both transitions and rewards, to the partially observable MDP (Section~\ref{sec:pomdp}), where the agent must act on incomplete information maintained through Bayesian belief updates. The multi-armed bandit (Section~\ref{sec:mab}) isolates the exploration--exploitation tradeoff in its purest form, and the restless bandit (Section~\ref{sec:restless}) reintroduces state dynamics by allowing arms to evolve whether or not they are played.

Several unifying ideas recur across these models. The \emph{Bellman equation}---the principle that an optimal policy decomposes into an optimal immediate action followed by optimal continuation---appears in Sections~\ref{sec:mdp}, \ref{sec:pomdp}, and (implicitly, via the Gittins index) Section~\ref{sec:mab}. \emph{Sufficient statistics} play a central role: the state in an MDP, the belief in a POMDP, and the posterior in a Bayesian bandit each compress all relevant history into a compact representation. The tension between \emph{tractability and generality} is a recurring theme: each extension in the hierarchy (adding actions, partial observability, or restless dynamics) increases modeling power at the cost of computational complexity, from polynomial-time solvability for MDPs to PSPACE-hardness for POMDPs and restless bandits.

The worked examples throughout the paper---the two-state Markov chain, the binary MDP and asset maintenance problem, the Tiger Problem, the three-armed bandit under four strategies, and the machine maintenance restless bandit---are intended to make the formal definitions concrete. Each example is accompanied by explicit numerical computation and, where appropriate, Python code for verification. Our hope is that these examples lower the barrier to entry for practitioners encountering these models for the first time.

Section~\ref{sec:beyond} surveyed several important model classes that lie outside the scope of this paper, including hidden Markov models, contextual bandits, Markov games, constrained MDPs, semi-Markov decision processes, and model-free reinforcement learning. Together with the models treated here, they span much of the landscape of Markov-based sequential decision-making. Exploring these extensions---and the connections among them---remains a rich area of both theoretical research and practical application.

\section*{Glossary}

\begin{description}

\item[Action] A decision available to the agent at each time step. In an MDP, the set of actions is $\mathcal{A} = \{a_1, \dots, a_m\}$. (Section~\ref{sec:mdp})

\item[Asymptotic optimality] A property of a policy whose performance approaches the optimum as a problem parameter (e.g., $K$ or $T$) tends to infinity. The Whittle index policy is asymptotically optimal under indexability and stability conditions. (Section~\ref{sec:restless})

\item[Belief MDP] The reformulation of a POMDP as a fully observable MDP whose state is the belief vector $\mathbf{b}$. The Bellman equation carries over but ranges over a continuous state space. (Section~\ref{sec:pomdp})

\item[Belief state] A probability distribution $\mathbf{b}$ over the state space $\mathcal{S}$, representing the agent's uncertainty about the true state in a POMDP. Updated via Bayes' rule after each action and observation. (Section~\ref{sec:pomdp})

\item[Bellman equation] The recursive characterization of the optimal value function: $V^*(s_i) = \max_a [r(s_i, a) + \gamma \sum_j p_{ij}(a) V^*(s_j)]$. Expresses the principle of optimality. (Section~\ref{sec:mdp})

\item[Constrained MDP (CMDP)] An MDP augmented with side constraints on expected cumulative costs. The optimal policy is generally a mixture of deterministic policies. (Section~\ref{sec:beyond})

\item[Contextual bandit] A bandit model in which the agent observes a context (feature vector) before selecting an arm, and the reward distribution depends on the context. Intermediate between the context-free bandit and the full MDP. (Section~\ref{sec:beyond})

\item[Cumulative regret] The total expected reward lost by not always pulling the best arm: $R_T = \sum_{i:\mu_i < \mu^*} \Delta_i \cdot \mathbb{E}[T_i(T)]$. (Section~\ref{sec:mab})

\item[Discount factor] A parameter $\gamma \in [0,1)$ that weights future rewards relative to immediate ones. A reward received $t$ steps in the future is worth $\gamma^t$ times its face value. (Section~\ref{sec:mdp})

\item[Ergodicity] A Markov chain is ergodic if it is irreducible (every state is reachable from every other) and aperiodic (does not cycle with a fixed period). An ergodic chain converges to a unique stationary distribution. (Section~\ref{sec:mc})

\item[Exploration] The act of trying actions or arms to gather information about their rewards or transition dynamics, at the cost of foregoing the currently best-known option. Contrasted with exploitation. (Section~\ref{sec:mab})

\item[Exploitation] The act of choosing the action or arm currently believed to be best, based on available information. Contrasted with exploration. (Section~\ref{sec:mab})

\item[EXP3] Exponential-weight algorithm for Exploration and Exploitation. A bandit strategy for the adversarial (non-stochastic) setting, achieving $O(\sqrt{KT \ln K})$ regret. (Section~\ref{sec:mab})

\item[Gap] The suboptimality of arm $i$: $\Delta_i = \mu^* - \mu_i$. Arms with smaller gaps are harder to distinguish from the optimum. (Section~\ref{sec:mab})

\item[Gittins index] The index $G_i$ assigned to each arm in the discounted Bayesian bandit, computed from the arm's posterior alone. The Bayes-optimal policy pulls the arm with the highest Gittins index. (Section~\ref{sec:mab})

\item[Hidden Markov model (HMM)] A Markov chain with unobservable states and state-dependent observations. The passive, uncontrolled counterpart of the POMDP. (Section~\ref{sec:beyond})

\item[Immediate reward] The reward $r(s_i, a)$ received when action $a$ is taken in state $s_i$. (Section~\ref{sec:mdp})

\item[Indexability] A property of a restless bandit arm: the set of states in which the passive action is optimal grows monotonically as the subsidy for passivity $\lambda$ increases. Required for the Whittle index to be well-defined. (Section~\ref{sec:restless})

\item[KL-UCB] A variant of UCB that uses Kullback--Leibler divergence instead of Hoeffding bounds, achieving asymptotically optimal regret with exact constants. (Section~\ref{sec:mab})

\item[Lai--Robbins lower bound] The information-theoretic lower bound on regret: $\liminf_{T \to \infty} \mathbb{E}[R_T] / \ln T \geq \sum_i \Delta_i / \text{KL}(\nu_i, \nu^*)$. Establishes that logarithmic regret growth is the best achievable. (Section~\ref{sec:mab})

\item[Markov chain] See Markov process. (Section~\ref{sec:mc})

\item[Markov decision process (MDP)] A controlled stochastic process defined by $(\mathcal{S}, \mathcal{A}, p, r, \gamma)$, extending the Markov chain with actions and rewards. (Section~\ref{sec:mdp})

\item[Markov game] A multi-agent extension of the MDP in which multiple players choose actions simultaneously and each receives a potentially different reward. Also called a stochastic game. (Section~\ref{sec:beyond})

\item[Markov process] A stochastic process satisfying the Markov property: the future state depends only on the present state, not on the history. Also called a Markov chain. (Section~\ref{sec:mc})

\item[Markov property] The memoryless property: $P(X_{t+1} = s_j \mid X_t = s_i, X_{t-1}, \dots, X_0) = P(X_{t+1} = s_j \mid X_t = s_i)$. (Section~\ref{sec:mc})

\item[Model-free reinforcement learning] Solution methods (e.g., Q-learning, policy gradient) that learn to act without a model of transition probabilities or rewards, using only sampled experience. (Section~\ref{sec:beyond})

\item[Multi-armed bandit (MAB)] A sequential decision problem in which an agent repeatedly chooses among $K$ arms with unknown reward distributions, balancing exploration and exploitation. A degenerate MDP with a single state. (Section~\ref{sec:mab})

\item[Observation function] The function $O(o \mid s', a)$ giving the probability of receiving observation $o$ after taking action $a$ and arriving in state $s'$. (Section~\ref{sec:pomdp})

\item[Optimal policy] The policy $\pi^*$ that achieves the maximum value at every state: $\pi^*(s_i) = \arg\max_a [r(s_i, a) + \gamma \sum_j p_{ij}(a) V^*(s_j)]$. (Section~\ref{sec:mdp})

\item[Optimal value function] The function $V^*(s_i) = \max_\pi V^\pi(s_i)$ giving the maximum achievable discounted return from each state. (Section~\ref{sec:mdp})

\item[Partially observable MDP (POMDP)] An MDP in which the agent cannot observe the true state and instead receives noisy observations. Defined by $(\mathcal{S}, \mathcal{A}, p, r, \Omega, O, \gamma)$. (Section~\ref{sec:pomdp})

\item[Policy] A rule $\pi : \mathcal{S} \to \mathcal{A}$ specifying which action to take in each state. Under a fixed policy, an MDP reduces to a Markov chain. (Section~\ref{sec:mdp})

\item[PSPACE-hard] A complexity class indicating that a problem is at least as hard as any problem solvable in polynomial space. Exact finite-horizon POMDP planning and the restless bandit problem are both PSPACE-hard. (Sections~\ref{sec:pomdp}, \ref{sec:restless})

\item[Restless bandit] A multi-armed bandit in which every arm transitions at every time step, regardless of whether it is played. Introduced by Whittle (1988). (Section~\ref{sec:restless})

\item[Semi-Markov decision process (SMDP)] An MDP in which actions have variable, random durations. Rewards accrue over the holding period and discounting accounts for elapsed time. (Section~\ref{sec:beyond})

\item[Stationary distribution] A distribution $\boldsymbol{\mu}^*$ satisfying $\boldsymbol{\mu}^* = \boldsymbol{\mu}^* \mathbf{P}$; once reached, it remains unchanged under further transitions. (Section~\ref{sec:mc})

\item[Successive elimination] A bandit algorithm that maintains a set of active arms and permanently discards arms whose confidence bounds prove them suboptimal. (Section~\ref{sec:mab})

\item[Sufficient statistic] A quantity that captures all information from the history relevant to future decisions. The state in an MDP, the belief in a POMDP, and the posterior in a Bayesian bandit are each sufficient statistics. (Sections~\ref{sec:mc}--\ref{sec:mab})

\item[Thompson Sampling] A Bayesian bandit strategy that samples from the posterior distribution of each arm's mean and pulls the arm with the highest sample. (Section~\ref{sec:mab})

\item[Transition matrix] The matrix $\mathbf{P}$ with entries $p_{ij}$, where each row sums to one. Governs the evolution of the state distribution: $\boldsymbol{\mu}_{t+1} = \boldsymbol{\mu}_t \mathbf{P}$. (Section~\ref{sec:mc})

\item[Transition probability] The probability $p_{ij}$ (or $p_{ij}(a)$ in an MDP) of moving from state $s_i$ to state $s_j$ (under action $a$). (Sections~\ref{sec:mc}, \ref{sec:mdp})

\item[UCB1] Upper Confidence Bound algorithm. Selects the arm maximizing $\hat{\mu}_i + \sqrt{2 \ln t / N_i}$, balancing exploitation (empirical mean) with exploration (confidence bonus). (Section~\ref{sec:mab})

\item[Value function] A function $V : \mathcal{S} \to \mathbb{R}$ assigning to each state the total discounted reward obtainable under a given policy (or the optimal policy). (Section~\ref{sec:mdp})

\item[Value iteration] An algorithm that repeatedly applies the Bellman operator to converge to the optimal value function. (Section~\ref{sec:mdp})

\item[Whittle index] The critical subsidy $W(s) = \inf\{\lambda : \text{passive is optimal in state } s\}$. Measures the value of activating a restless bandit arm in state $s$. (Section~\ref{sec:restless})

\item[Whittle index policy] The heuristic that activates the $M$ arms with the highest Whittle indices at each time step. Asymptotically optimal under indexability and stability. (Section~\ref{sec:restless})

\end{description}

\begin{thebibliography}{99}

\bibitem{markov1906}
  \foreignlanguage{russian}{.~.~,
  <<     ,    >>,
  \emph{. .-. . . -}, 2- ., .~15,
  .~135--156, 1906.}
  [A.~A.~Markov, ``Extension of the law of large numbers to dependent quantities,''
  \emph{Izvestiia Fiz.-Matem. Obshch. Kazan.\ Univ.}, 2nd Ser., vol.~15,
  pp.~135--156, 1906.]

\bibitem{bellman1957}
  R.~Bellman,
  ``A Markovian decision process,''
  RAND Corporation, Santa Monica, CA, Paper P-1066, April 1957.

\bibitem{astrom1965}
  K.~J.~\AA str\"om,
  ``Optimal control of Markov processes with incomplete state information~I,''
  \emph{Journal of Mathematical Analysis and Applications},
  vol.~10, pp.~174--205, 1965.

\bibitem{smallwood1973}
  R.~D.~Smallwood and E.~J.~Sondik,
  ``The optimal control of partially observable Markov processes over a finite horizon,''
  \emph{Operations Research},
  vol.~21, no.~5, pp.~1071--1088, 1973.

\bibitem{papadimitriou1987}
  C.~H.~Papadimitriou and J.~N.~Tsitsiklis,
  ``The complexity of Markov decision processes,''
  \emph{Mathematics of Operations Research},
  vol.~12, no.~3, pp.~441--450, 1987.

\bibitem{kaelbling1998}
  L.~P.~Kaelbling, M.~L.~Littman, and A.~R.~Cassandra,
  ``Planning and acting in partially observable stochastic domains,''
  \emph{Artificial Intelligence},
  vol.~101, nos.~1--2, pp.~99--134, 1998.

\bibitem{thompson1933}
  W.~R.~Thompson,
  ``On the likelihood that one unknown probability exceeds another in view of the evidence of two samples,''
  \emph{Biometrika},
  vol.~25, nos.~3--4, pp.~285--294, 1933.

\bibitem{robbins1952}
  H.~Robbins,
  ``Some aspects of the sequential design of experiments,''
  \emph{Bulletin of the American Mathematical Society},
  vol.~58, pp.~527--535, 1952.

\bibitem{gittins1979}
  J.~C.~Gittins,
  ``Bandit processes and dynamic allocation indices,''
  \emph{Journal of the Royal Statistical Society: Series~B},
  vol.~41, no.~2, pp.~148--177, 1979.

\bibitem{lai1985}
  T.~L.~Lai and H.~Robbins,
  ``Asymptotically efficient adaptive allocation rules,''
  \emph{Advances in Applied Mathematics},
  vol.~6, pp.~4--22, 1985.

\bibitem{auer2002}
  P.~Auer, N.~Cesa-Bianchi, and P.~Fischer,
  ``Finite-time analysis of the multiarmed bandit problem,''
  \emph{Machine Learning},
  vol.~47, pp.~235--256, 2002.

\bibitem{auer2002nonstochastic}
  P.~Auer, N.~Cesa-Bianchi, Y.~Freund, and R.~E.~Schapire,
  ``The nonstochastic multiarmed bandit problem,''
  \emph{SIAM Journal on Computing},
  vol.~32, no.~1, pp.~48--77, 2002.

\bibitem{evendar2006}
  E.~Even-Dar, S.~Mannor, and Y.~Mansour,
  ``Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems,''
  \emph{Journal of Machine Learning Research},
  vol.~7, pp.~1079--1105, 2006.

\bibitem{garivier2011}
  A.~Garivier and O.~Capp\'e,
  ``The KL-UCB algorithm for bounded stochastic bandits and beyond,''
  in \emph{Proceedings of the 24th Annual Conference on Learning Theory (COLT)},
  pp.~359--376, 2011.

\bibitem{whittle1988}
  P.~Whittle,
  ``Restless bandits: activity allocation in a changing world,''
  \emph{Journal of Applied Probability},
  vol.~25A, pp.~287--298, 1988.

\bibitem{papadimitriou1999}
  C.~H.~Papadimitriou and J.~N.~Tsitsiklis,
  ``The complexity of optimal queuing network control,''
  \emph{Mathematics of Operations Research},
  vol.~24, no.~2, pp.~293--305, 1999.

\bibitem{weber1990}
  R.~R.~Weber and G.~Weiss,
  ``On an index policy for restless bandits,''
  \emph{Journal of Applied Probability},
  vol.~27, no.~3, pp.~637--648, 1990.

\bibitem{rabiner1989}
  L.~R.~Rabiner,
  ``A tutorial on hidden Markov models and selected applications in speech recognition,''
  \emph{Proceedings of the IEEE},
  vol.~77, no.~2, pp.~257--286, 1989.

\bibitem{langford2008}
  J.~Langford and T.~Zhang,
  ``The Epoch-Greedy algorithm for contextual multi-armed bandits,''
  in \emph{Advances in Neural Information Processing Systems~20 (NIPS)},
  pp.~817--824, 2008.

\bibitem{shapley1953}
  L.~S.~Shapley,
  ``Stochastic games,''
  \emph{Proceedings of the National Academy of Sciences},
  vol.~39, no.~10, pp.~1095--1100, 1953.

\bibitem{bernstein2002}
  D.~S.~Bernstein, R.~Givan, N.~Immerman, and S.~Zilberstein,
  ``The complexity of decentralized control of Markov decision processes,''
  \emph{Mathematics of Operations Research},
  vol.~27, no.~4, pp.~819--840, 2002.

\bibitem{altman1999}
  E.~Altman,
  \emph{Constrained Markov Decision Processes}.
  Boca Raton, FL: Chapman \& Hall/CRC, 1999.

\bibitem{puterman1994}
  M.~L.~Puterman,
  \emph{Markov Decision Processes: Discrete Stochastic Dynamic Programming}.
  New York: Wiley, 1994.

\bibitem{sutton2018}
  R.~S.~Sutton and A.~G.~Barto,
  \emph{Reinforcement Learning: An Introduction},
  2nd~ed.
  Cambridge, MA: MIT Press, 2018.

\end{thebibliography}

\end{document}
